{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MissOh DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authored by haeyong.kang\n",
    "# date : 2020/06/21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AnotherMissOh Visual Structure\n",
    "- json_data['file_name'] : 'AnotherMissOh01.mp4'\n",
    "- json_data['visual_results']\n",
    "- json_data['visual_results'][0].keys() : dict_keys(['start_time', 'end_time', 'vid', 'image_info'])\n",
    "- {\n",
    "'start_time': '00:02:51;16', \n",
    "'end_time': '00:02:54;15', \n",
    "'vid': 'AnotherMissOh01_001_0078', \n",
    "'image_info': ...}\n",
    "- json_data['visual_results'][0]['image_info']\n",
    "- [{'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004295', \n",
    "'place': 'none', \n",
    "'persons': [\n",
    "{'person_id': 'Haeyoung1', \n",
    "'person_info': {\n",
    "'face_rect': {'min_x': 515, 'min_y': 0, 'max_x': 845, 'max_y': 443}, \n",
    "'full_rect': {'min_x': 278, 'min_y': 2, 'max_x': 1025, 'max_y': 769}, \n",
    "'behavior': 'stand up', \n",
    "'predicate': 'none', \n",
    "'emotion': 'Neutral', \n",
    "'face_rect_score': '0.5', \n",
    "'full_rect_score': '0.9'}, \n",
    "'related_objects': []}], \n",
    "'objects': []}, \n",
    "- {'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004311', \n",
    "'place': '', \n",
    "'persons': [{\n",
    "'person_id':'Haeyoung1',\n",
    "'person_info': {\n",
    "'face_rect': {'min_x': 515, 'min_y': 0, 'max_x': 831, 'max_y': 411}, \n",
    "'full_rect': {'min_x': 270, 'min_y': 0, 'max_x': 1025, 'max_y': 768}, \n",
    "'behavior': 'stand up', \n",
    "'predicate': 'none', \n",
    "'emotion': 'Neutral', \n",
    "'face_rect_score': '0.5', \n",
    "'full_rect_score': '0.9'}, \n",
    "'related_objects': []}],\n",
    "'objects': []},]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") # go to parent dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import json\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Yolo_v2_pytorch.src.utils import *\n",
    "from graphviz import Digraph, Graph\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_pydot import read_dot\n",
    "#from networkx.drawing.nx_agraph import read_dot\n",
    "from networkx.readwrite import json_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function networkx.drawing.nx_pydot.read_dot(path)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_blank(s):\n",
    "    return bool(s and s.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person\n",
      "(39, 129, 113)\n"
     ]
    }
   ],
   "source": [
    "MissOh_CLASSES = ['person']\n",
    "print(MissOh_CLASSES[0])\n",
    "global colors\n",
    "colors = pickle.load(open(\"../Yolo_v2_pytorch/src/pallete\", \"rb\"))\n",
    "print(colors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        \"You Only Look Once:Unified, Real-Time Object Detection\")\n",
    "    parser.add_argument(\"--image_size\", type=int,\n",
    "                        default=448,\n",
    "                        help=\"The common width and height for all images\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1,\n",
    "                        help=\"The number of images per batch\")\n",
    "    # Training base Setting\n",
    "    parser.add_argument(\"--momentum\", type=float, default=0.9)\n",
    "    parser.add_argument(\"--decay\", type=float, default=0.0005)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--num_epoches\", type=int, default=100)\n",
    "    parser.add_argument(\"--test_interval\", type=int, default=1,\n",
    "                        help=\"Number of epoches between testing phases\")\n",
    "    parser.add_argument(\"--object_scale\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--noobject_scale\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--class_scale\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--coord_scale\", type=float, default=5.0)\n",
    "    parser.add_argument(\"--reduction\", type=int, default=32)\n",
    "    parser.add_argument(\"--es_min_delta\", type=float, default=0.0,\n",
    "                        help=\"Early stopping's parameter:minimum change loss to qualify as an improvement\")\n",
    "    parser.add_argument(\"--es_patience\", type=int, default=0,\n",
    "                        help=\"Early stopping's parameter:number of epochs with no improvement after which training will be stopped. Set to 0 to disable this technique.\")\n",
    "\n",
    "    parser.add_argument(\"--pre_trained_model_type\",\n",
    "                        type=str, choices=[\"model\", \"params\"],\n",
    "                        default=\"model\")\n",
    "    parser.add_argument(\"--pre_trained_model_path\", type=str,\n",
    "                        default=\"Yolo_v2_pytorch/trained_models/only_params_trained_yolo_voc\") # Pre-training path\n",
    "\n",
    "    parser.add_argument(\"--saved_path\", type=str,\n",
    "                        default=\"./checkpoint\") # saved training path\n",
    "    parser.add_argument(\"--conf_threshold\", type=float, default=0.35)\n",
    "    parser.add_argument(\"--nms_threshold\", type=float, default=0.5)\n",
    "    args = parser.parse_args(args=[]) # for jupyter \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, class_scale=1.0, conf_threshold=0.35, coord_scale=5.0, decay=0.0005, dropout=0.5, es_min_delta=0.0, es_patience=0, image_size=448, momentum=0.9, nms_threshold=0.5, noobject_scale=0.5, num_epoches=100, object_scale=1.0, pre_trained_model_path='Yolo_v2_pytorch/trained_models/only_params_trained_yolo_voc', pre_trained_model_type='model', reduction=32, saved_path='./checkpoint', test_interval=1)\n"
     ]
    }
   ],
   "source": [
    "opt = get_args()\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the images and labels\n",
    "height, width = (768, 1024)\n",
    "width_ratio = 448 / width\n",
    "height_ratio = 448 / height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnotherMissOh(Dataset):\n",
    "    def __init__(self, dataset, img_path, json_path, display_log=True):\n",
    "        \n",
    "        self.display_log = display_log\n",
    "        self.init_clips(img_path)\n",
    "        self.load_json(dataset,img_path, json_path)\n",
    "        \n",
    "    def init_clips(self, img_path):\n",
    "        self.cnt_clips = 0\n",
    "        self.img_path = img_path\n",
    "        \n",
    "        self.img_size = (1024, 768)\n",
    "        self.img_scaled_size = (448, 448)\n",
    "        \n",
    "        tform = [\n",
    "            Resize(self.img_scaled_size), # should match to Yolo_V2\n",
    "            ToTensor(), \n",
    "            #Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # should match to Yolo_V2\n",
    "        ]\n",
    "        self.transformations = Compose(tform)\n",
    "        \n",
    "        '''\n",
    "        clips = {\n",
    "            'episode' : [],\n",
    "            'clip' : [],\n",
    "            'start_time' : [],\n",
    "            'end_time' : [],\n",
    "            'vid' : [],\n",
    "            'img_size' : [],\n",
    "            'img_scaled_size' : [],\n",
    "            'image_info' : []}\n",
    "            \n",
    "        image_info = {\n",
    "            'frame_id': [],\n",
    "            'place' : [],\n",
    "            'persons' : []}\n",
    "            \n",
    "        persons = {\n",
    "            'person_id': [],\n",
    "            'face_rect' : [],\n",
    "            'full_rect' : [],\n",
    "            'behavior' : [],\n",
    "            'predicate' : [],\n",
    "            'emotion' : [],\n",
    "            'face_rect_score' : [],\n",
    "            'full_rect_score' : []}\n",
    "        '''\n",
    "        \n",
    "    def load_json(self, dataset, img_path, json_path):\n",
    "        \n",
    "        self.clips = []\n",
    "        \n",
    "        for episode in dataset:\n",
    "            img_dir = img_path + 'AnotherMissOh{:02}/'.format(episode)\n",
    "            json_dir = json_path + 'AnotherMissOh{:02}_ver3.2.json'.format(episode)\n",
    "            if self.display_log:\n",
    "                print('imag_dir:{}'.format(img_dir))\n",
    "                print('json_dir:{}'.format(json_dir))\n",
    "\n",
    "            with open(json_dir, encoding='utf-8') as json_file:\n",
    "                json_data = json.load(json_file)\n",
    "\n",
    "            for i in range(len(json_data['visual_results'])):\n",
    "                clip = {}\n",
    "                clip['episode'] = []\n",
    "                clip['clip'] = []\n",
    "                clip['start_time'] = []\n",
    "                clip['end_time'] = []\n",
    "                clip['vid'] = []\n",
    "                clip['image_info'] = []\n",
    "                \n",
    "                if self.display_log:\n",
    "                    print(\"***{}th episode***{}th clips***************************************\".format(episode, i))\n",
    "                    print(\"['visual_results'][{}]['start_time']:{}\".format(i,json_data['visual_results'][i]['start_time']))\n",
    "                    print(\"['visual_results'][{}]['end_time']:{}\".format(i,json_data['visual_results'][i]['end_time']))\n",
    "                    print(\"['visual_results'][{}]['vid']:{}\".format(i,json_data['visual_results'][i]['vid'].replace('_', '/')))\n",
    "                    print(\"['visual_results'][{}]['img_size']:{}\".format(i,img_size))\n",
    "                    print(\"['visual_results'][{}]['img_scaled_size']:{}\".format(i,img_scaled_size))\n",
    "                    print(\"['visual_results'][{}]['episode']:{}\".format(i,episode))\n",
    "                \n",
    "                clip['episode'].append(episode)\n",
    "                clip['clip'].append(i)\n",
    "                clip['start_time'].append(json_data['visual_results'][i]['start_time'])\n",
    "                clip['end_time'].append(json_data['visual_results'][i]['end_time'])\n",
    "                clip['vid'].append(json_data['visual_results'][i]['vid'].replace('_', '/'))\n",
    "                \n",
    "                \n",
    "                for j, info in enumerate(json_data['visual_results'][i]['image_info']):\n",
    "                    image_info = {}\n",
    "                    image_info['frame_id'] = []\n",
    "                    image_info['place'] = []\n",
    "                    image_info['objects'] = {}\n",
    "                    image_info['persons'] = {}\n",
    "                    \n",
    "                    if self.display_log:\n",
    "                        print(\"=============={}th frame========================================\".format(j))\n",
    "                        \n",
    "                    img_file = img_dir + json_data['visual_results'][i]['vid'].replace('_', '/')[-8:] + '/'+ info['frame_id'][-16:] + '.jpg'\n",
    "                    image_info['frame_id'].append(img_file)\n",
    "                    image_info['place'].append(info['place'])\n",
    "                    \n",
    "                    image_info['objects']['object_id']=[]\n",
    "                    image_info['objects']['object_rect']=[]\n",
    "                    for k, obj in enumerate(info['objects']):\n",
    "                        image_info['objects']['object_id'].append(obj['object_id'])\n",
    "                        object_bbox = obj['object_rect']\n",
    "                        if (object_bbox['min_y'] == \"\" \n",
    "                            or object_bbox['max_y'] == \"\" \n",
    "                            or object_bbox['min_x'] == \"\" \n",
    "                            or object_bbox['max_x'] == \"\"):\n",
    "                            object_rect = []\n",
    "                            continue\n",
    "                        else:\n",
    "                            object_rect = [object_bbox['min_x'], object_bbox['min_y'], \n",
    "                                           object_bbox['max_x'], object_bbox['max_y']]\n",
    "                        image_info['objects']['object_rect'].append(object_rect)\n",
    "                    \n",
    "                    image_info['persons']['person_id']=[]\n",
    "                    image_info['persons']['face_rect']=[]\n",
    "                    image_info['persons']['full_rect']=[]\n",
    "                    image_info['persons']['behavior']=[]\n",
    "                    image_info['persons']['predicate']=[]\n",
    "                    image_info['persons']['emotion']=[]\n",
    "                    image_info['persons']['face_rect_score']=[]\n",
    "                    image_info['persons']['full_rect_score']=[]\n",
    "                    \n",
    "                    image_info['persons']['related_object_id']=[]\n",
    "                    image_info['persons']['related_object_rect']=[]\n",
    "                    \n",
    "                    for k, person in enumerate(info['persons']):\n",
    "                        if self.display_log:\n",
    "                            print(\"--------------------{}th person-----------------------------\".format(k))\n",
    "                            \n",
    "                        image_info['persons']['person_id'].append(person['person_id'])\n",
    "                        \n",
    "                        #import pdb; pdb.set_trace()\n",
    "                        for j, robj in enumerate(person['related_objects']):\n",
    "                            image_info['persons']['related_object_id'].append(robj['related_object_id'])\n",
    "                            robj_bbox = robj['related_object_rect']\n",
    "                            if (robj_bbox['min_y'] == \"\" \n",
    "                                or robj_bbox['max_y'] == \"\" \n",
    "                                or robj_bbox['min_x'] == \"\" \n",
    "                                or robj_bbox['max_x'] == \"\"):\n",
    "                                related_object_rect = []\n",
    "                                continue\n",
    "                            else:\n",
    "                                related_object_rect = [robj_bbox['min_x'], robj_bbox['min_y'], \n",
    "                                                       robj_bbox['max_x'], robj_bbox['max_y']]\n",
    "                            image_info['persons']['related_object_rect'].append(related_object_rect)\n",
    "    \n",
    "                        face_bbox = person['person_info']['face_rect']\n",
    "                        if (face_bbox['min_y'] == \"\" \n",
    "                            or face_bbox['max_y'] == \"\" \n",
    "                            or face_bbox['min_x'] == \"\" \n",
    "                            or face_bbox['max_x'] == \"\"):\n",
    "                            face_rect = []\n",
    "                            continue\n",
    "                        else:\n",
    "                            face_rect = [face_bbox['min_x'], face_bbox['min_y'], face_bbox['max_x'], face_bbox['max_y']]\n",
    "                        image_info['persons']['face_rect'].append(face_rect)\n",
    "                        full_bbox = person['person_info']['full_rect']\n",
    "                        if (full_bbox['min_y'] == \"\" \n",
    "                            or full_bbox['max_y'] == \"\" \n",
    "                            or full_bbox['min_x'] == \"\" \n",
    "                            or full_bbox['max_x'] == \"\"):\n",
    "                            full_rect = []\n",
    "                            continue\n",
    "                        else:\n",
    "                            full_rect = [full_bbox['min_x'], full_bbox['min_y'], full_bbox['max_x'], full_bbox['max_y']]\n",
    "                        image_info['persons']['full_rect'].append(full_rect)\n",
    "                        image_info['persons']['behavior'].append(person['person_info']['behavior'])\n",
    "                        image_info['persons']['predicate'].append(person['person_info']['predicate'])\n",
    "                        image_info['persons']['emotion'].append(person['person_info']['emotion'])\n",
    "                        image_info['persons']['face_rect_score'].append(person['person_info']['face_rect_score'])\n",
    "                        image_info['persons']['full_rect_score'].append(person['person_info']['full_rect_score'])\n",
    "                        \n",
    "                    clip['image_info'].append(image_info)\n",
    "                self.clips.append(clip)\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.clips)\n",
    "                    \n",
    "    def __getitem__(self, item):\n",
    "        info = self.clips[item]['image_info']\n",
    "        episode = self.clips[item]['episode']\n",
    "        clip = self.clips[item]['clip']\n",
    "        start_time = self.clips[item]['start_time']\n",
    "        end_time = self.clips[item]['start_time']\n",
    "        \n",
    "        images = []\n",
    "        for it, frame in enumerate(info):\n",
    "            img = Image.open(frame['frame_id'][0]).convert('RGB')\n",
    "            img = self.transformations(img)\n",
    "            images.append(img)\n",
    "        \n",
    "        return images, info, episode, clip, start_time, end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '../data/AnotherMissOh/AnotherMissOh_images/'\n",
    "json_path = '../data/AnotherMissOh/AnotherMissOh_Visual_ver3.2/'\n",
    "\n",
    "episode = 1\n",
    "train = [episode]\n",
    "train_set = AnotherMissOh(train, img_path, json_path, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(episode, scene, frm,st,et,info, save_file, debug = False):\n",
    "    import string\n",
    "    strseq = string.ascii_uppercase\n",
    "    \n",
    "    # define  graph\n",
    "    dot = Digraph('G',filename='{}.gv'.format(save_file),engine='fdp')\n",
    "    dot.attr('graph', rotate = '0', dpi='600',rankdir='TB', size='10,8')\n",
    "    dot.attr('node', height='0.1', fontsize='6')\n",
    "    dot.attr('edge', fontsize='6')\n",
    "\n",
    "    place = \"{}\".format(info['place'][0])\n",
    "    sound = \"{}\".format('sound')\n",
    "    \n",
    "    if not is_not_blank(place):\n",
    "        place = 'none'\n",
    "    if not is_not_blank(sound):\n",
    "        sound = 'none'\n",
    "        \n",
    "    num_of_persons = len(info['persons']['person_id'])\n",
    "    num_of_objects = len(info['objects']['object_id'])\n",
    "    \n",
    "    frm_graph = 'episode_{}_scene_{}_frame_{}'.format(\n",
    "        episode, scene, frm)\n",
    "    \n",
    "    #dot.node(frm_graph, style='filled', color='lightgrey')\n",
    "    episode_node = \"episode_{:02d}\".format(episode)\n",
    "    scene_node = \"scene_{:03d}\".format(scene)\n",
    "    frame_node = \"frame_{:04d}\".format(frm)\n",
    "    dot.node(episode_node, style='filled', color='lightgrey')\n",
    "    dot.node(scene_node, style='filled', color='lightgrey')\n",
    "    dot.node(frame_node, style='filled', color='lightgrey')\n",
    "    \n",
    "    dot.node(place, style='filled', color='lightblue')\n",
    "    dot.node(sound, style='filled', color='lightblue')\n",
    "    \n",
    "    if is_not_blank(episode_node) and is_not_blank(scene_node):\n",
    "        dot.edge(episode_node, scene_node)\n",
    "    \n",
    "    if is_not_blank(scene_node) and is_not_blank(frame_node):\n",
    "        dot.edge(scene_node, frame_node)\n",
    "        \n",
    "    if is_not_blank(frame_node) and is_not_blank(place):\n",
    "        dot.edge(frame_node, place)\n",
    "    \n",
    "    if is_not_blank(frame_node) and is_not_blank(sound):\n",
    "        dot.edge(frame_node, sound)\n",
    "    \n",
    "    for p in range(num_of_objects):\n",
    "        try: \n",
    "            object_id = info['objects']['object_id'][p]\n",
    "        except:\n",
    "            object_id = 'none'\n",
    "            #continue\n",
    "            \n",
    "        if is_not_blank(object_id) and object_id is not 'person':\n",
    "            dot.node(object_id, style='filled', color='gold')\n",
    "        if is_not_blank(frame_node) and is_not_blank(object_id):\n",
    "            dot.edge(frame_node, object_id)\n",
    "        \n",
    "    for p in range(num_of_persons):\n",
    "        \n",
    "        try:\n",
    "            person_id = \"{}\".format(info['persons']['person_id'][p])\n",
    "        except:\n",
    "            person_id = 'none'\n",
    "            #continue\n",
    "        try:\n",
    "            behavior = \"{}\".format(info['persons']['behavior'][p])\n",
    "        except:\n",
    "            person_id = 'none'\n",
    "            #continue\n",
    "        try:\n",
    "            predicate = \"{}\".format(info['persons']['predicate'][p])\n",
    "        except:\n",
    "            person_id = 'none'\n",
    "            #continue\n",
    "        try:\n",
    "            emotion = \"{}\".format(info['persons']['emotion'][p])\n",
    "        except:\n",
    "            person_id = 'none'\n",
    "            #continue\n",
    "        try:\n",
    "            robj_id = \"{}\".format(info['persons']['related_object_id'][p])\n",
    "        except:\n",
    "            robj_id = ''\n",
    "            #continue\n",
    "\n",
    "\n",
    "        if is_not_blank(person_id):\n",
    "            dot.node(person_id)\n",
    "        if is_not_blank(behavior):\n",
    "            dot.node(behavior, style='filled', color='green')\n",
    "        #if is_not_blank(predicate):\n",
    "        #    dot.node(predicate, style='filled', color='yellow')\n",
    "        if is_not_blank(emotion):\n",
    "            dot.node(emotion, style='filled', color='blue')\n",
    "\n",
    "        if is_not_blank(frame_node) and is_not_blank(person_id):\n",
    "            dot.edge(frame_node, person_id)\n",
    "        if is_not_blank(person_id) and is_not_blank(behavior):\n",
    "            dot.edge(person_id, behavior)\n",
    "        if is_not_blank(person_id) and is_not_blank(predicate) and is_not_blank(robj_id):\n",
    "            dot.edge(person_id, robj_id, label=predicate, color='red')\n",
    "            #dot.edge(predicate, robj_id)\n",
    "        if is_not_blank(person_id) and is_not_blank(emotion):\n",
    "            dot.edge(person_id, emotion)\n",
    "            \n",
    "        \n",
    "            \n",
    "    # show in image\n",
    "    dot.format = 'png'\n",
    "    dot.render('{}.gv'.format(save_file), view=True)\n",
    "\n",
    "    graph = cv2.imread('{}.gv.png'.format(save_file))\n",
    "    graph = cv2.resize(graph, dsize=(0, 0), fx=600.0/graph.shape[0], fy=600.0/graph.shape[0])\n",
    "\n",
    "    if debug:\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.imshow(graph)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '../results/drama_graph/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1249\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:1, scene:0, frame:0 st:['00:02:51;16'], et:['00:02:51;16']\n",
      "place:none\n",
      "num_of_persons:1\n",
      "person_id:Haeyoung1\n",
      "face_rect:[515, 0, 845, 443]\n",
      "full_rect:[278, 2, 1025, 769]\n",
      "behavior:stand up\n",
      "predicate:none\n",
      "emotion:Neutral\n",
      "episode:1, scene:0, frame:1 st:['00:02:51;16'], et:['00:02:51;16']\n",
      "place:\n",
      "num_of_persons:1\n",
      "person_id:Haeyoung1\n",
      "face_rect:[515, 0, 831, 411]\n",
      "full_rect:[270, 0, 1025, 768]\n",
      "behavior:stand up\n",
      "predicate:none\n",
      "emotion:Neutral\n",
      "episode:1, scene:0, frame:2 st:['00:02:51;16'], et:['00:02:51;16']\n",
      "place:\n",
      "num_of_persons:1\n",
      "person_id:Haeyoung1\n",
      "face_rect:[515, 0, 829, 442]\n",
      "full_rect:[255, 0, 1022, 767]\n",
      "behavior:stand up\n",
      "predicate:none\n",
      "emotion:Neutral\n",
      "episode:1, scene:0, frame:3 st:['00:02:51;16'], et:['00:02:51;16']\n",
      "place:\n",
      "num_of_persons:1\n",
      "person_id:Haeyoung1\n",
      "face_rect:[514, 0, 836, 438]\n",
      "full_rect:[255, 0, 1022, 767]\n",
      "behavior:stand up\n",
      "predicate:none\n",
      "emotion:Neutral\n",
      "episode:1, scene:0, frame:4 st:['00:02:51;16'], et:['00:02:51;16']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-8380691afb4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mnum_of_persons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfrm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'persons'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'person_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mnum_of_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfrm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'objects'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'object_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscene\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0met\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfrm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# read dot graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-16159edb0958>\u001b[0m in \u001b[0;36mgraph\u001b[0;34m(episode, scene, frm, st, et, info, save_file, debug)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# show in image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}.gv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}.gv.png'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/th0.3/lib/python3.6/site-packages/graphviz/files.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, filename, directory, view, cleanup, format, renderer, formatter, quiet, quiet_view)\u001b[0m\n\u001b[1;32m    207\u001b[0m         rendered = backend.render(self._engine, format, filepath,\n\u001b[1;32m    208\u001b[0m                                   \u001b[0mrenderer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                                   quiet=quiet)\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/th0.3/lib/python3.6/site-packages/graphviz/backend.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mcwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquiet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrendered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/th0.3/lib/python3.6/site-packages/graphviz/backend.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cmd, input, capture_output, check, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mquiet\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/th0.3/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communication_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/th0.3/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   1532\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1534\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1535\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/th0.3/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "\n",
    "for scene in range(len(train_set)):\n",
    "    #if scene < 1033:\n",
    "    #    continue\n",
    "        \n",
    "    images, info, episode, scene, st, et = train_set[scene]\n",
    "    scene= scene[0]\n",
    "    episode= episode[0]\n",
    "\n",
    "    for frm in range(len(images)):\n",
    "        image = images[frm].cpu().numpy()\n",
    "        #print(image)\n",
    "        imageInfo = cv2.cvtColor(np.transpose(image * 255,(1,2,0)), \n",
    "                                 cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        frm_name = \"episode_{:02d}_scene_{:03d}_frame_{:04d}\".format(episode, scene, frm)\n",
    "        save_file = save_dir + frm_name\n",
    "        print(\"episode:{}, scene:{}, frame:{} st:{}, et:{}\".format(\n",
    "            episode, scene, frm, st, et))\n",
    "\n",
    "        place = info[frm]['place'][0]\n",
    "        sound = 'sound'\n",
    "        num_of_persons = len(info[frm]['persons']['person_id'])\n",
    "        num_of_objects = len(info[frm]['objects']['object_id'])\n",
    "        graph(episode, scene, frm, st, et ,info[frm], save_file, debug)\n",
    "\n",
    "        # read dot graph\n",
    "        #dot_graph = nx.nx_pydot.read_dot('{}.gv'.format(save_file))\n",
    "        #s_graph = json.dumps(json_graph.node_link_data(dot_graph))\n",
    "        for p in range(num_of_objects):\n",
    "            try: \n",
    "                object_id = info[frm]['objects']['object_id'][p]\n",
    "            except:\n",
    "                object_id = 'none'\n",
    "                continue\n",
    "            try:\n",
    "                object_rect= info[frm]['objects']['object_rect'][p]\n",
    "            except:\n",
    "                object_rect = 'none'\n",
    "                continue\n",
    "\n",
    "\n",
    "            print(\"object_id:{}\".format(object_id))\n",
    "            print(\"object_rect:{}\".format(object_rect))\n",
    "\n",
    "            xmin_ = int(max(object_rect[0] * width_ratio, 0))\n",
    "            ymin_ = int(max(object_rect[1] * height_ratio, 0))\n",
    "            xmax_ = int(min((object_rect[2]) * width_ratio, 448))\n",
    "            ymax_ = int(min((object_rect[3]) * height_ratio, 448))\n",
    "            cv2.rectangle(imageInfo, (xmin_, ymin_), (xmax_, ymax_), \n",
    "                          (0.0, 128.0, 255.0), 2)\n",
    "\n",
    "            cv2.putText(imageInfo, object_id, (xmin_, ymin_), \n",
    "                        cv2.FONT_HERSHEY_PLAIN, 1.5, (255.0, 255.0, 255.0), 1)\n",
    "\n",
    "\n",
    "        for p in range(num_of_persons):\n",
    "            try: \n",
    "                person_id = info[frm]['persons']['person_id'][p]\n",
    "            except:\n",
    "                person_id = 'none'\n",
    "                #continue\n",
    "            try:\n",
    "                face_rect= info[frm]['persons']['face_rect'][p]\n",
    "            except:\n",
    "                face_rect = 'none'\n",
    "                #continue\n",
    "            try: \n",
    "                full_rect = info[frm]['persons']['full_rect'][p]\n",
    "            except:\n",
    "                full_rect = 'none'\n",
    "                #continue\n",
    "            try: \n",
    "                behavior = info[frm]['persons']['behavior'][p]\n",
    "            except:\n",
    "                behavior = 'none'\n",
    "                #continue\n",
    "            try:\n",
    "                predicate = info[frm]['persons']['predicate'][p]\n",
    "            except:\n",
    "                predicate = 'none'\n",
    "                #continue\n",
    "            try: \n",
    "                emotion = info[frm]['persons']['emotion'][p]\n",
    "            except:\n",
    "                emotion = 'none'\n",
    "                #continue\n",
    "\n",
    "            print(\"place:{}\".format(place))\n",
    "            print(\"num_of_persons:{}\".format(num_of_persons))\n",
    "            print(\"person_id:{}\".format(person_id))\n",
    "            print(\"face_rect:{}\".format(face_rect))\n",
    "            print(\"full_rect:{}\".format(full_rect))\n",
    "            print(\"behavior:{}\".format(behavior))\n",
    "            print(\"predicate:{}\".format(predicate))\n",
    "            print(\"emotion:{}\".format(emotion))\n",
    "\n",
    "            # face rect\n",
    "            xmin = int(max(face_rect[0] * width_ratio, 0))\n",
    "            ymin = int(max(face_rect[1] * height_ratio, 0))\n",
    "            xmax = int(min((face_rect[2]) * width_ratio, 448))\n",
    "            ymax = int(min((face_rect[3]) * height_ratio, 448))\n",
    "            cv2.rectangle(imageInfo, (xmin, ymin), (xmax, ymax), colors[0], 2)\n",
    "\n",
    "            # full rect\n",
    "            xmin = int(max(full_rect[0] * width_ratio, 0))\n",
    "            ymin = int(max(full_rect[1] * height_ratio, 0))\n",
    "            xmax = int(min((full_rect[2]) * width_ratio, 448))\n",
    "            ymax = int(min((full_rect[3]) * height_ratio, 448))\n",
    "            cv2.rectangle(imageInfo, (xmin, ymin), (xmax, ymax), colors[2], 2)\n",
    "            cv2.putText(imageInfo, person_id, (xmin, ymin), \n",
    "                        cv2.FONT_HERSHEY_PLAIN, 1.5, (255.0, 255.0, 255.0), 1)\n",
    "            cv2.putText(imageInfo, emotion, (xmin, ymax+20), \n",
    "                        cv2.FONT_HERSHEY_PLAIN, 1, (0.0, 0.0, 255.0), 1)\n",
    "            cv2.putText(imageInfo, behavior, (xmin, ymax+30), \n",
    "                        cv2.FONT_HERSHEY_PLAIN, 1, (0.0, 255.0, 0.0), 1)\n",
    "            cv2.putText(imageInfo, place, (10, 20), \n",
    "                        cv2.FONT_HERSHEY_PLAIN, 1.5, (255.0, 255.0, 255.0), 1)\n",
    "            cv2.putText(imageInfo, sound, (10, 40), \n",
    "                        cv2.FONT_HERSHEY_PLAIN, 1.5, (255.0, 255.0, 255.0), 1)\n",
    "\n",
    "        imageInfo = cv2.resize(imageInfo,dsize=(1024, 768))\n",
    "        cv2.imwrite(\"{}_frame.png\".format(save_file), imageInfo)\n",
    "        imageInfo = cv2.cvtColor(imageInfo, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if debug:\n",
    "            plt.figure(figsize=(8,8))\n",
    "            plt.imshow(np.uint8(imageInfo))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"directed\": true, \"multigraph\": true, \n",
    " \"graph\": {\"name\": \"G\", \"node\": {\"fontsize\": \"7\", \"height\": \"0.1\"}, \n",
    "           \"edge\": {\"fontsize\": \"7\"}}, \n",
    " \"nodes\": [{\"color\": \"lightgrey\", \"style\": \"filled\", \"id\": \"episode_1\"}, \n",
    "           {\"color\": \"lightgrey\", \"style\": \"filled\", \"id\": \"scene_10\"}, \n",
    "           {\"color\": \"lightgrey\", \"style\": \"filled\", \"id\": \"frame_0\"}, \n",
    "           {\"color\": \"lightblue\", \"style\": \"filled\", \"id\": \"kitchen\"}, \n",
    "           {\"color\": \"lightblue\", \"style\": \"filled\", \"id\": \"sound\"}, \n",
    "           {\"id\": \"Jeongsuk\"}, \n",
    "           {\"color\": \"green\", \"style\": \"filled\", \"id\": \"cook\"}, \n",
    "           {\"color\": \"blue\", \"style\": \"filled\", \"id\": \"Happiness\"}, \n",
    "           {\"id\": \"Deogi\"}], \n",
    " \"links\": [{\"source\": \"Jeongsuk\", \"target\": \"cook\", \"key\": 0}, \n",
    "           {\"source\": \"Jeongsuk\", \"target\": \"Happiness\", \"key\": 0}, \n",
    "           {\"source\": \"Deogi\", \"target\": \"cook\", \"key\": 0}, \n",
    "           {\"source\": \"Deogi\", \"target\": \"Happiness\", \"key\": 0}]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
