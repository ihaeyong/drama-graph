{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MissOh DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AnotherMissOh Visual Structure\n",
    "- json_data['file_name'] : 'AnotherMissOh01.mp4'\n",
    "- json_data['visual_results']\n",
    "- json_data['visual_results'][0].keys() : dict_keys(['start_time', 'end_time', 'vid', 'image_info'])\n",
    "- {\n",
    "'start_time': '00:02:51;16', \n",
    "'end_time': '00:02:54;15', \n",
    "'vid': 'AnotherMissOh01_001_0078', \n",
    "'image_info': ...}\n",
    "- json_data['visual_results'][0]['image_info']\n",
    "- [{'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004295', \n",
    "'place': 'none', \n",
    "'persons': [\n",
    "{'person_id': 'Haeyoung1', \n",
    "'person_info': {\n",
    "'face_rect': {'min_x': 515, 'min_y': 0, 'max_x': 845, 'max_y': 443}, \n",
    "'full_rect': {'min_x': 278, 'min_y': 2, 'max_x': 1025, 'max_y': 769}, \n",
    "'behavior': 'stand up', \n",
    "'predicate': 'none', \n",
    "'emotion': 'Neutral', \n",
    "'face_rect_score': '0.5', \n",
    "'full_rect_score': '0.9'}, \n",
    "'related_objects': []}], \n",
    "'objects': []}, \n",
    "- {'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004311', \n",
    "'place': '', \n",
    "'persons': [{\n",
    "'person_id':'Haeyoung1',\n",
    "'person_info': {\n",
    "'face_rect': {'min_x': 515, 'min_y': 0, 'max_x': 831, 'max_y': 411}, \n",
    "'full_rect': {'min_x': 270, 'min_y': 0, 'max_x': 1025, 'max_y': 768}, \n",
    "'behavior': 'stand up', \n",
    "'predicate': 'none', \n",
    "'emotion': 'Neutral', \n",
    "'face_rect_score': '0.5', \n",
    "'full_rect_score': '0.9'}, \n",
    "'related_objects': []}],\n",
    "'objects': []},]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get install graphviz xdg-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"../\") # go to parent dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import json\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Yolo_v2_pytorch.src.utils import *\n",
    "from graphviz import Digraph, Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_blank(s):\n",
    "    return bool(s and s.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person\n",
      "(39, 129, 113)\n"
     ]
    }
   ],
   "source": [
    "MissOh_CLASSES = ['person']\n",
    "print(MissOh_CLASSES[0])\n",
    "global colors\n",
    "colors = pickle.load(open(\"../Yolo_v2_pytorch/src/pallete\", \"rb\"))\n",
    "print(colors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"../\") # go to parent dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, conf_threshold=0.35, data_path_test='./Yolo_v2_pytorch/missoh_test/', display=False, emo_net_ch=64, image_size=448, img_path='./data/AnotherMissOh/AnotherMissOh_images_ver3.2/', json_path='./data/AnotherMissOh/AnotherMissOh_Visual_ver3.2/', model='baseline', nms_threshold=0.5, pre_trained_model_type='model', saved_path='./checkpoint/refined_models')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "from Yolo_v2_pytorch.src.utils import *\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from Yolo_v2_pytorch.src.yolo_net import Yolo\n",
    "from Yolo_v2_pytorch.src.anotherMissOh_dataset import AnotherMissOh, Splits, SortFullRect, PersonCLS,PBeHavCLS, FaceCLS, ObjectCLS, P2ORelCLS\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from lib.place_model import place_model, label_mapping, accuracy, label_remapping, place_buffer\n",
    "from lib.person_model import person_model\n",
    "from lib.behavior_model import behavior_model\n",
    "from lib.pytorch_misc import optimistic_restore, de_chunkize, clip_grad_norm, flatten\n",
    "from lib.focal_loss import FocalLossWithOneHot, FocalLossWithOutOneHot, CELossWithOutOneHot\n",
    "from lib.face_model import face_model\n",
    "from lib.object_model import object_model\n",
    "from lib.relation_model import relation_model\n",
    "from lib.emotion_model import emotion_model, crop_face_emotion, EmoCLS\n",
    "\n",
    "num_persons = len(PersonCLS)\n",
    "num_behaviors = len(PBeHavCLS)\n",
    "num_faces = len(FaceCLS)\n",
    "num_objects = len(ObjectCLS)\n",
    "num_relations = len(P2ORelCLS)\n",
    "num_emos = len(EmoCLS)\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        \"You Only Look Once: Unified, Real-Time Object Detection\")\n",
    "    parser.add_argument(\"--image_size\",\n",
    "                        type=int, default=448,\n",
    "                        help=\"The common width and height for all images\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1,\n",
    "                        help=\"The number of images per batch\")\n",
    "    parser.add_argument(\"--conf_threshold\",\n",
    "                        type=float, default=0.35)\n",
    "    parser.add_argument(\"--nms_threshold\",\n",
    "                        type=float, default=0.5)\n",
    "    parser.add_argument(\"--pre_trained_model_type\",\n",
    "                        type=str, choices=[\"model\", \"params\"],\n",
    "                        default=\"model\")\n",
    "    parser.add_argument(\"--data_path_test\",\n",
    "                        type=str,\n",
    "                        default=\"./Yolo_v2_pytorch/missoh_test/\",\n",
    "                        help=\"the root folder of dataset\")\n",
    "\n",
    "    parser.add_argument(\"--saved_path\", type=str,\n",
    "                        default=\"./checkpoint/refined_models\")\n",
    "\n",
    "    parser.add_argument(\"--img_path\", type=str,\n",
    "                        default=\"./data/AnotherMissOh/AnotherMissOh_images_ver3.2/\")\n",
    "    parser.add_argument(\"--json_path\", type=str,\n",
    "                        default=\"./data/AnotherMissOh/AnotherMissOh_Visual_ver3.2/\")\n",
    "    parser.add_argument(\"-model\", dest='model', type=str, default=\"baseline\")\n",
    "    parser.add_argument(\"-display\", dest='display', action='store_true')\n",
    "    parser.add_argument(\"-emo_net_ch\", dest='emo_net_ch',type=int, default=64)\n",
    "    args = parser.parse_args([])\n",
    "    return args\n",
    "\n",
    "# get args.\n",
    "opt = get_args()\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.img_path = \"../data/AnotherMissOh/AnotherMissOh_images_ver3.2/\"\n",
    "opt.json_path = \"../data/AnotherMissOh/AnotherMissOh_Visual_ver3.2/\"\n",
    "opt.saved_path = \"../checkpoint/refined_models\"\n",
    "opt.display = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tform = [\n",
    "    Resize((448, 448)),  # should match to Yolo_V2\n",
    "    ToTensor(),\n",
    "    # Normalize(# should match to Yolo_V2\n",
    "    #mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "]\n",
    "transf = Compose(tform)\n",
    "\n",
    "# splits the episodes int train, val, test\n",
    "train, val, test = Splits(num_episodes=18)\n",
    "\n",
    "# load datasets\n",
    "train_set = AnotherMissOh(train, opt.img_path, opt.json_path, False)\n",
    "val_set = AnotherMissOh(val, opt.img_path, opt.json_path, False)\n",
    "test_set = AnotherMissOh(test, opt.img_path, opt.json_path, False)\n",
    "\n",
    "# model path\n",
    "model_path = \"{}/anotherMissOh_{}.pth\".format(\n",
    "    opt.saved_path,opt.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "loaded with ../checkpoint/refined_models/anotherMissOh_only_params_emotion_integration.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "emotion_model(\n",
       "  (yolo_net): YoloD(\n",
       "    (stage1_conv1): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (stage1_conv2): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (stage1_conv3): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (stage1_conv4): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (stage1_conv5): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (stage1_conv6): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (stage1_conv7): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (stage1_conv8): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (stage1_conv9): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (stage1_conv10): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (stage1_conv11): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (stage1_conv12): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (stage1_conv13): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (stage2_a_maxpl): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (stage2_a_conv1): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (stage2_a_conv2): Sequential(\n",
       "      (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (stage2_a_conv3): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (stage2_a_conv4): Sequential(\n",
       "      (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (stage2_a_conv5): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (stage2_a_conv6): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (stage2_a_conv7): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (stage2_b_conv): Sequential(\n",
       "      (0): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (stage3_conv1): Sequential(\n",
       "      (0): Conv2d(1280, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "  )\n",
       "  (emo_branch): Sequential(\n",
       "    (0): Conv2d(1024, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(64, 7, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(123)\n",
    "    device = torch.cuda.current_device()\n",
    "else:\n",
    "    torch.manual_seed(123)\n",
    "\n",
    "# set test loader params\n",
    "test_params = {\"batch_size\": opt.batch_size,\n",
    "               \"shuffle\": False,\n",
    "               \"drop_last\": False,\n",
    "               \"collate_fn\": custom_collate_fn}\n",
    "\n",
    "# set test loader\n",
    "test_loader = DataLoader(test_set, **test_params)\n",
    "\n",
    "# ---------------(1) load refined models --------------------\n",
    "# get the trained models from\n",
    "# https://drive.google.com/drive/folders/1WXzP8nfXU4l0cNOtSPX9O1qxYH2m6LIp\n",
    "# emotion model\n",
    "if True:\n",
    "    model_emo = emotion_model(opt.emo_net_ch, num_persons, device)\n",
    "    trained_emotion = '../checkpoint/refined_models' + os.sep + \"{}\".format(\n",
    "    'anotherMissOh_only_params_emotion_integration.pth')\n",
    "    model_emo.load_state_dict(torch.load(trained_emotion))\n",
    "    print(\"loaded with {}\".format(trained_emotion))\n",
    "model_emo.cuda(device)\n",
    "model_emo.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the color map for detection results\n",
    "colors = pickle.load(open(\"../Yolo_v2_pytorch/src/pallete\", \"rb\"))\n",
    "\n",
    "width, height = (1024, 768)\n",
    "width_ratio = float(opt.image_size) / width\n",
    "height_ratio = float(opt.image_size) / height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/vtt_env/lib/python3.6/site-packages/ipykernel_launcher.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame.__len__10, mAP_file:AnotherMissOh07_001_0035_IMAGE_0000002438.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_001_0035_IMAGE_0000002446.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_001_0035_IMAGE_0000002454.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_001_0035_IMAGE_0000002462.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_001_0035_IMAGE_0000002470.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_001_0035_IMAGE_0000002478.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_001_0035_IMAGE_0000002486.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_001_0035_IMAGE_0000002494.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_001_0035_IMAGE_0000002502.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_001_0035_IMAGE_0000002510.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0036_IMAGE_0000002568.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0036_IMAGE_0000002576.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0036_IMAGE_0000002584.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0036_IMAGE_0000002592.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0036_IMAGE_0000002600.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0036_IMAGE_0000002608.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0036_IMAGE_0000002616.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0036_IMAGE_0000002680.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0036_IMAGE_0000002672.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0036_IMAGE_0000002664.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0037_IMAGE_0000002764.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0037_IMAGE_0000002772.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0037_IMAGE_0000002780.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0037_IMAGE_0000002788.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0037_IMAGE_0000002796.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0037_IMAGE_0000002804.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0037_IMAGE_0000002812.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0037_IMAGE_0000002820.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0037_IMAGE_0000002828.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0037_IMAGE_0000002836.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0038_IMAGE_0000002874.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0038_IMAGE_0000002882.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0038_IMAGE_0000002890.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0038_IMAGE_0000002898.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0038_IMAGE_0000002906.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0038_IMAGE_0000002914.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0038_IMAGE_0000002922.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0038_IMAGE_0000002930.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0038_IMAGE_0000002938.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0038_IMAGE_0000002946.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0039_IMAGE_0000003139.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0039_IMAGE_0000003147.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0039_IMAGE_0000003155.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0039_IMAGE_0000003163.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0039_IMAGE_0000003171.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0039_IMAGE_0000003179.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0039_IMAGE_0000003187.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0039_IMAGE_0000003195.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0039_IMAGE_0000003203.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_002_0039_IMAGE_0000003211.txt\n",
      "frame.__len__7, mAP_file:AnotherMissOh07_003_0043_IMAGE_0000004990.txt\n",
      "frame.__len__7, mAP_file:AnotherMissOh07_003_0043_IMAGE_0000004998.txt\n",
      "frame.__len__7, mAP_file:AnotherMissOh07_003_0043_IMAGE_0000005006.txt\n",
      "frame.__len__7, mAP_file:AnotherMissOh07_003_0043_IMAGE_0000005014.txt\n",
      "frame.__len__7, mAP_file:AnotherMissOh07_003_0043_IMAGE_0000005022.txt\n",
      "frame.__len__7, mAP_file:AnotherMissOh07_003_0043_IMAGE_0000005030.txt\n",
      "frame.__len__7, mAP_file:AnotherMissOh07_003_0043_IMAGE_0000005038.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0045_IMAGE_0000005132.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0045_IMAGE_0000005140.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0045_IMAGE_0000005148.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0045_IMAGE_0000005156.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0045_IMAGE_0000005164.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0045_IMAGE_0000005172.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0045_IMAGE_0000005180.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0045_IMAGE_0000005188.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0045_IMAGE_0000005196.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0045_IMAGE_0000005204.txt\n",
      "frame.__len__6, mAP_file:AnotherMissOh07_004_0047_IMAGE_0000005251.txt\n",
      "frame.__len__6, mAP_file:AnotherMissOh07_004_0047_IMAGE_0000005259.txt\n",
      "frame.__len__6, mAP_file:AnotherMissOh07_004_0047_IMAGE_0000005267.txt\n",
      "frame.__len__6, mAP_file:AnotherMissOh07_004_0047_IMAGE_0000005275.txt\n",
      "frame.__len__6, mAP_file:AnotherMissOh07_004_0047_IMAGE_0000005283.txt\n",
      "frame.__len__6, mAP_file:AnotherMissOh07_004_0047_IMAGE_0000005291.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0048_IMAGE_0000005337.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0048_IMAGE_0000005345.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0048_IMAGE_0000005353.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0048_IMAGE_0000005361.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0048_IMAGE_0000005369.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0048_IMAGE_0000005377.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0048_IMAGE_0000005385.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0048_IMAGE_0000005393.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0048_IMAGE_0000005401.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0048_IMAGE_0000005409.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0050_IMAGE_0000005632.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0050_IMAGE_0000005640.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0050_IMAGE_0000005648.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0050_IMAGE_0000005656.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0050_IMAGE_0000005664.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0050_IMAGE_0000005672.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0050_IMAGE_0000005680.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0050_IMAGE_0000005688.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0050_IMAGE_0000005696.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0050_IMAGE_0000005704.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0051_IMAGE_0000005771.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0051_IMAGE_0000005779.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0051_IMAGE_0000005787.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0051_IMAGE_0000005795.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0051_IMAGE_0000005803.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0051_IMAGE_0000005811.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0051_IMAGE_0000005819.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0051_IMAGE_0000005827.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0051_IMAGE_0000005835.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0051_IMAGE_0000005843.txt\n",
      "frame.__len__8, mAP_file:AnotherMissOh07_004_0052_IMAGE_0000005859.txt\n",
      "frame.__len__8, mAP_file:AnotherMissOh07_004_0052_IMAGE_0000005867.txt\n",
      "frame.__len__8, mAP_file:AnotherMissOh07_004_0052_IMAGE_0000005875.txt\n",
      "frame.__len__8, mAP_file:AnotherMissOh07_004_0052_IMAGE_0000005883.txt\n",
      "frame.__len__8, mAP_file:AnotherMissOh07_004_0052_IMAGE_0000005891.txt\n",
      "frame.__len__8, mAP_file:AnotherMissOh07_004_0052_IMAGE_0000005899.txt\n",
      "frame.__len__8, mAP_file:AnotherMissOh07_004_0052_IMAGE_0000005907.txt\n",
      "frame.__len__8, mAP_file:AnotherMissOh07_004_0052_IMAGE_0000005915.txt\n",
      "frame.__len__3, mAP_file:AnotherMissOh07_004_0054_IMAGE_0000005985.txt\n",
      "frame.__len__3, mAP_file:AnotherMissOh07_004_0054_IMAGE_0000005993.txt\n",
      "frame.__len__3, mAP_file:AnotherMissOh07_004_0054_IMAGE_0000006001.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0055_IMAGE_0000006045.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0055_IMAGE_0000006053.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0055_IMAGE_0000006061.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0055_IMAGE_0000006069.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0055_IMAGE_0000006077.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0055_IMAGE_0000006085.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0055_IMAGE_0000006093.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0055_IMAGE_0000006101.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0055_IMAGE_0000006109.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0055_IMAGE_0000006117.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0056_IMAGE_0000006235.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0056_IMAGE_0000006243.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0056_IMAGE_0000006251.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0056_IMAGE_0000006259.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0056_IMAGE_0000006267.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0056_IMAGE_0000006275.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0056_IMAGE_0000006283.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0056_IMAGE_0000006291.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0056_IMAGE_0000006299.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0056_IMAGE_0000006307.txt\n",
      "frame.__len__2, mAP_file:AnotherMissOh07_004_0057_IMAGE_0000006314.txt\n",
      "frame.__len__2, mAP_file:AnotherMissOh07_004_0057_IMAGE_0000006322.txt\n",
      "frame.__len__2, mAP_file:AnotherMissOh07_004_0058_IMAGE_0000006335.txt\n",
      "frame.__len__2, mAP_file:AnotherMissOh07_004_0058_IMAGE_0000006343.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0059_IMAGE_0000006372.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0059_IMAGE_0000006380.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0059_IMAGE_0000006388.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0059_IMAGE_0000006396.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0059_IMAGE_0000006404.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0059_IMAGE_0000006412.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0059_IMAGE_0000006420.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0059_IMAGE_0000006428.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0059_IMAGE_0000006436.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0059_IMAGE_0000006444.txt\n",
      "frame.__len__2, mAP_file:AnotherMissOh07_004_0060_IMAGE_0000006459.txt\n",
      "frame.__len__2, mAP_file:AnotherMissOh07_004_0060_IMAGE_0000006467.txt\n",
      "frame.__len__3, mAP_file:AnotherMissOh07_004_0061_IMAGE_0000006476.txt\n",
      "frame.__len__3, mAP_file:AnotherMissOh07_004_0061_IMAGE_0000006484.txt\n",
      "frame.__len__3, mAP_file:AnotherMissOh07_004_0061_IMAGE_0000006492.txt\n",
      "frame.__len__4, mAP_file:AnotherMissOh07_004_0062_IMAGE_0000006501.txt\n",
      "frame.__len__4, mAP_file:AnotherMissOh07_004_0062_IMAGE_0000006509.txt\n",
      "frame.__len__4, mAP_file:AnotherMissOh07_004_0062_IMAGE_0000006517.txt\n",
      "frame.__len__4, mAP_file:AnotherMissOh07_004_0062_IMAGE_0000006525.txt\n",
      "frame.__len__3, mAP_file:AnotherMissOh07_004_0063_IMAGE_0000006535.txt\n",
      "frame.__len__3, mAP_file:AnotherMissOh07_004_0063_IMAGE_0000006543.txt\n",
      "frame.__len__3, mAP_file:AnotherMissOh07_004_0063_IMAGE_0000006551.txt\n",
      "frame.__len__4, mAP_file:AnotherMissOh07_004_0064_IMAGE_0000006564.txt\n",
      "frame.__len__4, mAP_file:AnotherMissOh07_004_0064_IMAGE_0000006572.txt\n",
      "frame.__len__4, mAP_file:AnotherMissOh07_004_0064_IMAGE_0000006580.txt\n",
      "frame.__len__4, mAP_file:AnotherMissOh07_004_0064_IMAGE_0000006588.txt\n",
      "frame.__len__3, mAP_file:AnotherMissOh07_004_0065_IMAGE_0000006600.txt\n",
      "frame.__len__3, mAP_file:AnotherMissOh07_004_0065_IMAGE_0000006608.txt\n",
      "frame.__len__3, mAP_file:AnotherMissOh07_004_0065_IMAGE_0000006616.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0066_IMAGE_0000006625.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0066_IMAGE_0000006633.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0066_IMAGE_0000006641.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0066_IMAGE_0000006649.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0066_IMAGE_0000006657.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0066_IMAGE_0000006665.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0066_IMAGE_0000006673.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0066_IMAGE_0000006681.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0066_IMAGE_0000006689.txt\n",
      "frame.__len__10, mAP_file:AnotherMissOh07_004_0066_IMAGE_0000006697.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-91f9df1c2ff1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbuffer_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# load test clips\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/vtt_env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/vtt_env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/drama-graph/Yolo_v2_pytorch/src/anotherMissOh_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0mimg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'frame_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mimg_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/vtt_env/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    932\u001b[0m         \"\"\"\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/vtt_env/lib/python3.6/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Sequence buffers\n",
    "buffer_images = []\n",
    "# load test clips\n",
    "for iter, batch in enumerate(test_loader):\n",
    "    image, info = batch\n",
    "\n",
    "    # sort label info on fullrect\n",
    "    image, label, behavior_label, obj_label, face_label, emo_label, frame_id = SortFullRect(\n",
    "        image, info, is_train=False)\n",
    "\n",
    "    try :\n",
    "        image = torch.cat(image,0).cuda(device)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # -----------------(2) inference -------------------------\n",
    "    # emotion\n",
    "    if np.array(face_label).size > 0 and False:\n",
    "        face_label = [fl for fl in face_label if len(fl) > 0]\n",
    "        emo_label = [el for el in emo_label if len(el) > 0]\n",
    "        image_c = image.permute(0,2,3,1).cpu()\n",
    "        face_crops, emo_gt = crop_face_emotion(image_c, face_label, emo_label, opt)\n",
    "        face_crops, emo_gt = face_crops.cuda(device).contiguous(), emo_gt.cuda(device)\n",
    "        emo_logits = model_emo(face_crops)\n",
    "        num_img, num_face = np.array(face_label).shape[0:2]\n",
    "        emo_logits = emo_logits.view(num_img, num_face, 7)\n",
    "    \n",
    "    for idx, frame in enumerate(frame_id):\n",
    "\n",
    "        # ---------------(3) mkdir for evaluations----------------------\n",
    "        f_info = frame[0].split('/')\n",
    "        save_dir = './results/drama-graph/{}/{}/{}/'.format(\n",
    "            f_info[4], f_info[5], f_info[6])\n",
    "\n",
    "        f_file = f_info[7]\n",
    "        mAP_file = \"{}_{}_{}_{}\".format(f_info[4],\n",
    "                                        f_info[5],\n",
    "                                        f_info[6],\n",
    "                                        f_info[7].replace(\"jpg\", \"txt\"))\n",
    "        if opt.display:\n",
    "            print(\"frame.__len__{}, mAP_file:{}\".format(len(frame_id), mAP_file))\n",
    "            \n",
    "        # --------------(5) visualization of inferences ----------\n",
    "        # out of try : pdb.set_trace = lambda : None\n",
    "        try:\n",
    "            # for some empty video clips\n",
    "            img = image[idx]\n",
    "            # ToTensor function normalizes image pixel values into [0,1]\n",
    "            np_img = img.cpu().numpy()\n",
    "            np_img = np.transpose(np_img,(1,2,0)) * 255\n",
    "            output_image = cv2.cvtColor(np_img,cv2.COLOR_RGB2BGR)\n",
    "            output_image = cv2.resize(output_image, (width, height))\n",
    "\n",
    "            if len(predictions_p) != 0 :\n",
    "                prediction = predictions[idx]\n",
    "                \n",
    "                if False:\n",
    "                    b_logit = b_logits[idx]\n",
    "\n",
    "                # person and behavior\n",
    "                num_preds = len(prediction)\n",
    "                for jdx, pred in enumerate(prediction):\n",
    "                    # emotion\n",
    "                    if False:\n",
    "                        fl = face_label[idx][jdx]\n",
    "                        face_x0, face_y0 = int(fl[0]/width_ratio), int(fl[1]/height_ratio)\n",
    "                        face_x1, face_y1 = int(fl[2]/width_ratio), int(fl[3]/height_ratio)\n",
    "                        emo_ij = F.softmax(emo_logits[idx,jdx,:], dim=0).argmax().detach().cpu().numpy()\n",
    "                        emo_txt = EmoCLS[emo_ij]\n",
    "                        cv2.rectangle(output_image, (face_x0,face_y0),\n",
    "                                      (face_x1,face_y1), (255,255,0), 1)\n",
    "                        cv2.putText(output_image, emo_txt, (face_x0, face_y0-5),\n",
    "                                    cv2.FONT_HERSHEY_PLAIN, 1, (255,255,0), 1,\n",
    "                                    cv2.LINE_AA)\n",
    "                    \n",
    "                    if opt.display:\n",
    "                        print(\"detected {}\".format(save_dir + \"{}\".format(f_file)))\n",
    "                else:\n",
    "                    if opt.display:\n",
    "                        print(\"non-detected {}\".format(\n",
    "                        save_dir + \"{}\".format(f_file)))\n",
    "\n",
    "            # save output image  \n",
    "            cv2.imwrite(save_dir + \"{}\".format(f_file), output_image)\n",
    "            # save images\n",
    "            plt_output_image = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)\n",
    "            plt.imshow(plt_output_image.astype('uint8'))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # --------------(4) ground truth ---------------------------------\n",
    "        # save person ground truth\n",
    "        gt_person_cnt = 0\n",
    "        if len(label) > idx :\n",
    "            # person\n",
    "            for det in label[idx]:\n",
    "                cls = PersonCLS[int(det[4])]\n",
    "                xmin = str(max(det[0] / width_ratio, 0))\n",
    "                ymin = str(max(det[1] / height_ratio, 0))\n",
    "                xmax = str(min((det[2]) / width_ratio, width))\n",
    "                ymax = str(min((det[3]) / height_ratio, height))\n",
    "                cat_det = '%s %s %s %s %s\\n' % (cls, xmin, ymin, xmax, ymax)\n",
    "                if opt.display:\n",
    "                    print(\"person_gt:{}\".format(cat_det))\n",
    "                gt_person_cnt += 1\n",
    "                \n",
    "            # behavior\n",
    "            for j, det in enumerate(label[idx]):\n",
    "                cls = PBeHavCLS[int(behavior_label[idx][j])].replace(' ', '_')\n",
    "                if cls == 'none':\n",
    "                    continue\n",
    "\n",
    "                cls = cls.replace('/', '_')\n",
    "                xmin = str(max(det[0] / width_ratio, 0))\n",
    "                ymin = str(max(det[1] / height_ratio, 0))\n",
    "                xmax = str(min((det[2]) / width_ratio, width))\n",
    "                ymax = str(min((det[3]) / height_ratio, height))\n",
    "                cat_det = '%s %s %s %s %s\\n' % (cls, xmin, ymin, xmax, ymax)\n",
    "                if opt.display:\n",
    "                    print(\"behavior_gt:{}\".format(cat_det))\n",
    "        # object\n",
    "        gt_object_cnt = 0\n",
    "        if len(obj_label) > idx :\n",
    "            for det in obj_label[idx]:\n",
    "                cls = ObjectCLS[int(det[4])]\n",
    "                xmin = str(max(det[0] / width_ratio, 0))\n",
    "                ymin = str(max(det[1] / height_ratio, 0))\n",
    "                xmax = str(min((det[2]) / width_ratio, width))\n",
    "                ymax = str(min((det[3]) / height_ratio, height))\n",
    "                cat_det = '%s %s %s %s %s\\n' % (cls, xmin, ymin, xmax, ymax)\n",
    "                if opt.display:\n",
    "                    print(\"object_gt:{}\".format(cat_det))\n",
    "                gt_object_cnt += 1\n",
    "                \n",
    "        # relation\n",
    "        gt_relation_cnt = 0\n",
    "        if len(obj_label) > idx:\n",
    "            for det in obj_label[idx]:\n",
    "                cls = P2ORelCLS[int(det[5])]\n",
    "                xmin = str(max(det[0] / width_ratio, 0))\n",
    "                ymin = str(max(det[1] / height_ratio, 0))\n",
    "                xmax = str(min((det[2]) / width_ratio, width))\n",
    "                ymax = str(min((det[3]) / height_ratio, height))\n",
    "                cat_det = '%s %s %s %s %s\\n' % (cls, xmin, ymin, xmax, ymax)\n",
    "                if opt.display:\n",
    "                    print(\"relation_gt:{}\".format(cat_det))\n",
    "                gt_relation_cnt += 1\n",
    "\n",
    "        # place\n",
    "        if len(preds_place_txt) > idx:\n",
    "            if opt.display:\n",
    "                print(\"place_gt:{}\".format(target_place_txt[idx]))\n",
    "        # face\n",
    "        gt_face_cnt = 0\n",
    "        if len(face_label) > idx:\n",
    "            for det in face_label[idx]:\n",
    "                cls = PersonCLS[int(det[4])]\n",
    "                xmin = str(max(det[0] / width_ratio, 0))\n",
    "                ymin = str(max(det[1] / height_ratio, 0))\n",
    "                xmax = str(min((det[2]) / width_ratio, width))\n",
    "                ymax = str(min((det[3]) / height_ratio, height))\n",
    "                cat_det = '%s %s %s %s %s\\n' % (cls, xmin, ymin, xmax, ymax)\n",
    "                print(\"face_gt:{}\".format(cat_det))\n",
    "                gt_face_cnt += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
