{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MissOh DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AnotherMissOh Visual Structure\n",
    "- json_data['file_name'] : 'AnotherMissOh01.mp4'\n",
    "- json_data['visual_results']\n",
    "- json_data['visual_results'][0].keys() : dict_keys(['start_time', 'end_time', 'vid', 'image_info'])\n",
    "- {\n",
    "'start_time': '00:02:51;16', \n",
    "'end_time': '00:02:54;15', \n",
    "'vid': 'AnotherMissOh01_001_0078', \n",
    "'image_info': ...}\n",
    "- json_data['visual_results'][0]['image_info']\n",
    "- [{'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004295', \n",
    "'place': 'none', \n",
    "'persons': [\n",
    "{'person_id': 'Haeyoung1', \n",
    "'person_info': {\n",
    "'face_rect': {'min_x': 515, 'min_y': 0, 'max_x': 845, 'max_y': 443}, \n",
    "'full_rect': {'min_x': 278, 'min_y': 2, 'max_x': 1025, 'max_y': 769}, \n",
    "'behavior': 'stand up', \n",
    "'predicate': 'none', \n",
    "'emotion': 'Neutral', \n",
    "'face_rect_score': '0.5', \n",
    "'full_rect_score': '0.9'}, \n",
    "'related_objects': []}], \n",
    "'objects': []}, \n",
    "- {'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004311', \n",
    "'place': '', \n",
    "'persons': [{\n",
    "'person_id':'Haeyoung1',\n",
    "'person_info': {\n",
    "'face_rect': {'min_x': 515, 'min_y': 0, 'max_x': 831, 'max_y': 411}, \n",
    "'full_rect': {'min_x': 270, 'min_y': 0, 'max_x': 1025, 'max_y': 768}, \n",
    "'behavior': 'stand up', \n",
    "'predicate': 'none', \n",
    "'emotion': 'Neutral', \n",
    "'face_rect_score': '0.5', \n",
    "'full_rect_score': '0.9'}, \n",
    "'related_objects': []}],\n",
    "'objects': []},]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get install graphviz xdg-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"../\") # go to parent dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import json\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Yolo_v2_pytorch.src.utils import *\n",
    "from graphviz import Digraph, Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_blank(s):\n",
    "    return bool(s and s.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person\n",
      "(39, 129, 113)\n"
     ]
    }
   ],
   "source": [
    "MissOh_CLASSES = ['person']\n",
    "print(MissOh_CLASSES[0])\n",
    "global colors\n",
    "colors = pickle.load(open(\"../Yolo_v2_pytorch/src/pallete\", \"rb\"))\n",
    "print(colors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"../\") # go to parent dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, conf_threshold=0.35, data_path_test='./Yolo_v2_pytorch/missoh_test/', display=False, emo_net_ch=64, image_size=448, img_path='./data/AnotherMissOh/AnotherMissOh_images_ver3.2/', json_path='./data/AnotherMissOh/AnotherMissOh_Visual_ver3.2/', model='baseline', nms_threshold=0.5, pre_trained_model_type='model', saved_path='./checkpoint/refined_models')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "from Yolo_v2_pytorch.src.utils import *\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from Yolo_v2_pytorch.src.yolo_net import Yolo\n",
    "from Yolo_v2_pytorch.src.anotherMissOh_dataset import AnotherMissOh, Splits, SortFullRect, PersonCLS,PBeHavCLS, FaceCLS, ObjectCLS, P2ORelCLS\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from lib.place_model import place_model, label_mapping, accuracy, label_remapping, place_buffer\n",
    "from lib.person_model import person_model\n",
    "from lib.behavior_model import behavior_model\n",
    "from lib.pytorch_misc import optimistic_restore, de_chunkize, clip_grad_norm, flatten\n",
    "from lib.focal_loss import FocalLossWithOneHot, FocalLossWithOutOneHot, CELossWithOutOneHot\n",
    "from lib.face_model import face_model\n",
    "from lib.object_model import object_model\n",
    "from lib.relation_model import relation_model\n",
    "from lib.emotion_model import emotion_model, crop_face_emotion, EmoCLS\n",
    "\n",
    "num_persons = len(PersonCLS)\n",
    "num_behaviors = len(PBeHavCLS)\n",
    "num_faces = len(FaceCLS)\n",
    "num_objects = len(ObjectCLS)\n",
    "num_relations = len(P2ORelCLS)\n",
    "num_emos = len(EmoCLS)\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        \"You Only Look Once: Unified, Real-Time Object Detection\")\n",
    "    parser.add_argument(\"--image_size\",\n",
    "                        type=int, default=448,\n",
    "                        help=\"The common width and height for all images\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1,\n",
    "                        help=\"The number of images per batch\")\n",
    "    parser.add_argument(\"--conf_threshold\",\n",
    "                        type=float, default=0.35)\n",
    "    parser.add_argument(\"--nms_threshold\",\n",
    "                        type=float, default=0.5)\n",
    "    parser.add_argument(\"--pre_trained_model_type\",\n",
    "                        type=str, choices=[\"model\", \"params\"],\n",
    "                        default=\"model\")\n",
    "    parser.add_argument(\"--data_path_test\",\n",
    "                        type=str,\n",
    "                        default=\"./Yolo_v2_pytorch/missoh_test/\",\n",
    "                        help=\"the root folder of dataset\")\n",
    "\n",
    "    parser.add_argument(\"--saved_path\", type=str,\n",
    "                        default=\"./checkpoint/refined_models\")\n",
    "\n",
    "    parser.add_argument(\"--img_path\", type=str,\n",
    "                        default=\"./data/AnotherMissOh/AnotherMissOh_images_ver3.2/\")\n",
    "    parser.add_argument(\"--json_path\", type=str,\n",
    "                        default=\"./data/AnotherMissOh/AnotherMissOh_Visual_ver3.2/\")\n",
    "    parser.add_argument(\"-model\", dest='model', type=str, default=\"baseline\")\n",
    "    parser.add_argument(\"-display\", dest='display', action='store_true')\n",
    "    parser.add_argument(\"-emo_net_ch\", dest='emo_net_ch',type=int, default=64)\n",
    "    args = parser.parse_args([])\n",
    "    return args\n",
    "\n",
    "# get args.\n",
    "opt = get_args()\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8a29936d0186>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrawing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnx_pydot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_dot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#from networkx.drawing.nx_agraph import read_dot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadwrite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/vtt_env/lib/python3.6/site-packages/networkx/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelabel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/vtt_env/lib/python3.6/site-packages/networkx/generators/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcographs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunity\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegree_seq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirected\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/vtt_env/lib/python3.6/site-packages/networkx/generators/community.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Accommodates for both SciPy and non-SciPy implementations..\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzeta\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_zeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mzeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/vtt_env/lib/python3.6/site-packages/scipy/special/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msf_error\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpecialFunctionWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpecialFunctionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_ufuncs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_ufuncs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/vtt_env/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from networkx.drawing.nx_pydot import read_dot\n",
    "#from networkx.drawing.nx_agraph import read_dot\n",
    "from networkx.readwrite import json_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.img_path = \"../data/AnotherMissOh/AnotherMissOh_images_ver3.2/\"\n",
    "opt.json_path = \"../data/AnotherMissOh/AnotherMissOh_Visual_ver3.2/\"\n",
    "opt.saved_path = \"../checkpoint/refined_models\"\n",
    "opt.display = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tform = [\n",
    "    Resize((448, 448)),  # should match to Yolo_V2\n",
    "    ToTensor(),\n",
    "    # Normalize(# should match to Yolo_V2\n",
    "    #mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "]\n",
    "transf = Compose(tform)\n",
    "\n",
    "# splits the episodes int train, val, test\n",
    "train, val, test = Splits(num_episodes=18)\n",
    "\n",
    "# load datasets\n",
    "train_set = AnotherMissOh(train, opt.img_path, opt.json_path, False)\n",
    "val_set = AnotherMissOh(val, opt.img_path, opt.json_path, False)\n",
    "test_set = AnotherMissOh(test, opt.img_path, opt.json_path, False)\n",
    "\n",
    "episode = 7\n",
    "infer = [episode]\n",
    "infer_set = AnotherMissOh(infer, opt.img_path, opt.json_path, False)\n",
    "\n",
    "\n",
    "# model path\n",
    "model_path = \"{}/anotherMissOh_{}.pth\".format(\n",
    "    opt.saved_path,opt.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(123)\n",
    "    device = torch.cuda.current_device()\n",
    "else:\n",
    "    torch.manual_seed(123)\n",
    "\n",
    "# set test loader params\n",
    "test_params = {\"batch_size\": opt.batch_size,\n",
    "               \"shuffle\": False,\n",
    "               \"drop_last\": False,\n",
    "               \"collate_fn\": custom_collate_fn}\n",
    "\n",
    "# set test loader\n",
    "test_loader = DataLoader(infer_set, **test_params)\n",
    "\n",
    "# ---------------(1) load refined models --------------------\n",
    "# get the trained models from\n",
    "# https://drive.google.com/drive/folders/1WXzP8nfXU4l0cNOtSPX9O1qxYH2m6LIp\n",
    "# person and behavior\n",
    "if True :\n",
    "    # pre-trained behavior model\n",
    "    # step 1: person trained on voc 50 epoch\n",
    "    # step 2: person feature based behavior sequence learning 100 epoch\n",
    "    trained_persons = '../checkpoint/behavior' + os.sep + \"{}\".format(\n",
    "        'anotherMissOh_voc_person_behavior_new.pth')\n",
    "    model_p = torch.load(trained_persons)\n",
    "    print(\"loaded with person and behavior model {}\".format(trained_persons))\n",
    "model_p.cuda(device)\n",
    "model_p.eval()\n",
    "\n",
    "# face model\n",
    "if False:\n",
    "    model_face = face_model(num_persons, num_faces, device)\n",
    "    trained_face = '../checkpoint/refined_models' + os.sep + \"{}\".format(\n",
    "    'anotherMissOh_only_params_face.pth')\n",
    "    model_face.load_state_dict(torch.load(trained_face))\n",
    "    print(\"loaded with {}\".format(trained_face))\n",
    "else:\n",
    "    trained_face = '../checkpoint/face' + os.sep + \"{}\".format(\n",
    "    'anotherMissOh_face.pth')\n",
    "    model_face =torch.load(trained_face)\n",
    "    print(\"loaded with {}\".format(trained_face))\n",
    "    \n",
    "model_face.cuda(device)\n",
    "model_face.eval()\n",
    "\n",
    "# emotion model\n",
    "if True:\n",
    "    model_emo = emotion_model(opt.emo_net_ch, num_persons, device)\n",
    "    trained_emotion = '../checkpoint/refined_models' + os.sep + \"{}\".format(\n",
    "    'anotherMissOh_only_params_emotion_integration.pth')\n",
    "    model_emo.load_state_dict(torch.load(trained_emotion))\n",
    "    print(\"loaded with {}\".format(trained_emotion))\n",
    "model_emo.cuda(device)\n",
    "model_emo.eval()\n",
    "\n",
    "# object model\n",
    "if True:\n",
    "    # add model\n",
    "    model_object = object_model(num_objects)\n",
    "    trained_object = '../checkpoint/refined_models' + os.sep + \"{}\".format(\n",
    "    'anotherMissOh_only_params_object_integration.pth')\n",
    "    # model load\n",
    "    print(\"loaded with {}\".format(trained_object))\n",
    "    model_object.load_state_dict(torch.load(trained_object))\n",
    "\n",
    "model_object.cuda(device)\n",
    "model_object.eval()\n",
    "\n",
    "\n",
    "# relation model\n",
    "if True:\n",
    "    # add model\n",
    "    model_relation = relation_model(num_persons, num_objects, num_relations, opt, device)\n",
    "    trained_relation = '../checkpoint/refined_models' + os.sep + \"{}\".format(\n",
    "    'anotherMissOh_only_params_relation_integration.pth')\n",
    "    # model load\n",
    "    print(\"loaded with {}\".format(trained_relation))\n",
    "    model_relation.load_state_dict(torch.load(trained_relation))\n",
    "model_relation.cuda(device)\n",
    "model_relation.eval()\n",
    "\n",
    "# place model\n",
    "if True:\n",
    "    model_place = place_model(num_persons, num_behaviors, device)\n",
    "    # add model\n",
    "    trained_place = '../checkpoint/refined_models' + os.sep + \"{}\".format(\n",
    "        'anotherMissOh_only_params_place_integration.pth')\n",
    "    # model load\n",
    "    print(\"loaded with {}\".format(trained_place))\n",
    "    model_place.load_state_dict(torch.load(trained_place)['model'])\n",
    "model_place.cuda(device)\n",
    "model_place.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the color map for detection results\n",
    "colors = pickle.load(open(\"../Yolo_v2_pytorch/src/pallete\", \"rb\"))\n",
    "\n",
    "width, height = (1024, 768)\n",
    "width_ratio = float(opt.image_size) / width\n",
    "height_ratio = float(opt.image_size) / height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_blank(s):\n",
    "    return bool(s and s.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_to_json(episode, scene, frm, info, save_file=None):\n",
    "\n",
    "    if save_file is None:\n",
    "        save_file = 'temp_graph'\n",
    "    import string\n",
    "    strseq = string.ascii_uppercase\n",
    "\n",
    "    # define  graph\n",
    "    dot = Digraph('G',filename='{}.gv'.format(save_file),engine='fdp')\n",
    "    dot.attr('graph', rotate = '0', dpi='600',rankdir='TB', size='10,8')\n",
    "    dot.attr('node', height='0.1', fontsize='6')\n",
    "    dot.attr('edge', fontsize='6')\n",
    "\n",
    "    place = \"{}\".format(info['place'])\n",
    "    sound = \"{}\".format(info['sound'])\n",
    "\n",
    "    if not is_not_blank(place):\n",
    "        place = 'none'\n",
    "    if not is_not_blank(sound):\n",
    "        sound = 'none'\n",
    "\n",
    "    num_of_persons = len(info['persons'])\n",
    "    num_of_objects = len(info['objects'])\n",
    "\n",
    "    frm_graph = 'episode_{}_scene_{}_shot_{}'.format(\n",
    "        episode, scene, frm)\n",
    "\n",
    "    #dot.node(frm_graph, style='filled', color='lightgrey')\n",
    "    episode_node = \"episode_{:02d}\".format(episode)\n",
    "    scene_node = \"scene_{:03d}\".format(scene)\n",
    "    frame_node = \"shot_{:04d}\".format(frm)\n",
    "    dot.node(episode_node, style='filled', color='lightgrey')\n",
    "    dot.node(scene_node, style='filled', color='lightgrey')\n",
    "    dot.node(frame_node, style='filled', color='lightgrey')\n",
    "\n",
    "    # backgrounds--------------------------------------------\n",
    "    dot.node(place, style='filled', color='lightblue')\n",
    "    dot.node(sound, style='filled', color='lightblue')\n",
    "\n",
    "    if is_not_blank(episode_node) and is_not_blank(scene_node):\n",
    "        dot.edge(episode_node, scene_node)\n",
    "\n",
    "    if is_not_blank(scene_node) and is_not_blank(frame_node):\n",
    "        dot.edge(scene_node, frame_node)\n",
    "\n",
    "    if is_not_blank(frame_node) and is_not_blank(place):\n",
    "        dot.edge(frame_node, place)\n",
    "\n",
    "    if is_not_blank(frame_node) and is_not_blank(sound):\n",
    "        dot.edge(frame_node, sound)\n",
    "\n",
    "    # person ------------------------------------------------\n",
    "    for person_id in info['persons'].keys():\n",
    "\n",
    "        if is_not_blank(person_id):\n",
    "            dot.node(person_id)\n",
    "\n",
    "        # behavior---\n",
    "        if 'behavior' in info['persons'][person_id].keys():\n",
    "            behavior_id = info['persons'][person_id]['behavior']\n",
    "        else:\n",
    "            behavior_id = 'none'\n",
    "        if is_not_blank(behavior_id):\n",
    "            dot.node(behavior_id, style='filled', color='green')\n",
    "\n",
    "        # emotion---\n",
    "        if 'emotion' in info['persons'][person_id].keys():\n",
    "            emotion_id = info['persons'][person_id]['emotion']\n",
    "        else:\n",
    "            emotion_id = 'none'\n",
    "        if is_not_blank(emotion_id):\n",
    "            dot.node(emotion_id, style='filled', color='blue')\n",
    "\n",
    "        if is_not_blank(frame_node) and is_not_blank(person_id):\n",
    "            dot.edge(frame_node, person_id)\n",
    "\n",
    "        if is_not_blank(person_id) and is_not_blank(behavior_id):\n",
    "            dot.edge(person_id, behavior_id)\n",
    "\n",
    "        if is_not_blank(person_id) and is_not_blank(emotion_id):\n",
    "            dot.edge(person_id, emotion_id)\n",
    "\n",
    "    # relation ---------------------------------------------\n",
    "    for object_id in info['objects'].keys():\n",
    "        if is_not_blank(object_id):\n",
    "            dot.node(object_id, style='filled', color='gold')\n",
    "\n",
    "    for person_id in info['relations'].keys():\n",
    "        if person_id not in info['persons'].keys():\n",
    "            dot.node(person_id)\n",
    "            dot.edge(frame_node, person_id)\n",
    "\n",
    "        for object_id in info['relations'][person_id].keys() :\n",
    "            if object_id not in info['objects'].keys():\n",
    "                dot.node(object_id)\n",
    "                dot.edge(frame_node, object_id)\n",
    "            predicate = info['relations'][person_id][object_id]\n",
    "            dot.edge(person_id, object_id,label=predicate, color='red')\n",
    "\n",
    "    # convert dot graph to json\n",
    "    if False:\n",
    "        dot_to_json =json.dumps(json_graph.node_link_data(dot))\n",
    "    else:\n",
    "        dot_to_json = json.dumps(info)\n",
    "\n",
    "    with open('{}.json'.format(save_file), 'w') as f:\n",
    "        json.dump(dot_to_json, f)\n",
    "\n",
    "    # show in image\n",
    "    dot.format = 'png'\n",
    "    dot.render('{}.gv'.format(save_file), view=True) \n",
    "\n",
    "    graph = cv2.imread('{}.gv.png'.format(save_file))\n",
    "    graph = cv2.resize(graph, dsize=(0, 0), fx=600.0/graph.shape[0], fy=600.0/graph.shape[0])\n",
    "\n",
    "    if True:\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.imshow(graph)\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = 0\n",
    "frm = 0\n",
    "\n",
    "#---------------graph structure--------------------------\n",
    "graph_json = {}\n",
    "graph_json['persons'] = {}\n",
    "graph_json['objects'] = {}\n",
    "graph_json['relations'] = {}\n",
    "\n",
    "# ---------------persons---------------------------------\n",
    "graph_json['persons']['Haeyoung1'] = {}\n",
    "graph_json['persons']['Haeyoung1']['emotion'] = 'happy'\n",
    "graph_json['persons']['Haeyoung1']['behavior'] = 'talking'\n",
    "graph_json['persons']['Deogi'] = {}\n",
    "graph_json['persons']['Deogi']['emotion'] = 'happy'\n",
    "graph_json['persons']['Deogi']['behavior'] = 'eating'\n",
    "\n",
    "# ---------------objects---------------------------------\n",
    "graph_json['objects']['spoon'] = {}\n",
    "graph_json['objects']['spoon']['Deogi'] = 'N_R'\n",
    "\n",
    "# ---------------Relations-------------------------------\n",
    "graph_json['relations']['Deogi'] = {}\n",
    "graph_json['relations']['Deogi']['spoon'] = 'holding'\n",
    "\n",
    "# ---------------Backgrounds-----------------------------\n",
    "graph_json['place'] = 'kitchen'\n",
    "graph_json['sound'] = 'talking'\n",
    "\n",
    "info = graph_json\n",
    "print(info)\n",
    "\n",
    "graph_to_json(episode, clip, frm, graph_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sequence buffers\n",
    "buffer_images = []\n",
    "graph_info = {}\n",
    "# load test clips\n",
    "for iter, batch in enumerate(test_loader):\n",
    "    image, info = batch\n",
    "    \n",
    "    # sort label info on fullrect\n",
    "    image, label, behavior_label, obj_label, face_label, emo_label, frame_id = SortFullRect(\n",
    "        image, info, is_train=False)\n",
    "\n",
    "    try :\n",
    "        image = torch.cat(image,0).cuda(device)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # -----------------(2) inference -------------------------\n",
    "    # person and behavior predictions\n",
    "    # person\n",
    "    # logits : [1, 125, 14, 14]\n",
    "    if False:\n",
    "        p_logits, _ = model_p(image)\n",
    "        predictions_p = post_processing(p_logits,\n",
    "                                        opt.image_size,\n",
    "                                        PersonCLS,\n",
    "                                        model_p.detector.anchors,\n",
    "                                        opt.conf_threshold,\n",
    "                                        opt.nms_threshold)\n",
    "    \n",
    "    # logits : [1, 125, 14, 14]\n",
    "    # behavior_logits : [1, 135, 14, 14]\n",
    "    else :\n",
    "        predictions_p, b_logits = model_p(image, label, behavior_label)\n",
    "\n",
    "    # face\n",
    "    #if np.array(face_label).size > 0 :\n",
    "    face_logits = model_face(image)\n",
    "    predictions_face = post_processing(face_logits,\n",
    "                                        opt.image_size,\n",
    "                                        FaceCLS,\n",
    "                                        model_face.detector.anchors,\n",
    "                                        opt.conf_threshold,\n",
    "                                        opt.nms_threshold)\n",
    "    \n",
    "    if len(predictions_face) != 0:\n",
    "            num_preds = len(predictions_face)\n",
    "            num_face_per_pred = [len(pred) for pred in predictions_face]\n",
    "            image_c = image.permute(0,2,3,1).cpu()\n",
    "            face_crops, _ = crop_face_emotion(image_c, predictions_face, None, opt)\n",
    "            face_crops = face_crops.cuda(device).contiguous()\n",
    "            emo_logits_raw = model_emo(face_crops)\n",
    "\n",
    "            emo_logits, idx = [], 0\n",
    "            for pl in num_face_per_pred:\n",
    "                emo_logits.append(emo_logits_raw[idx:idx+pl])\n",
    "                idx = idx+pl\n",
    "\n",
    "    # object\n",
    "    if np.array(obj_label).size > 0 :\n",
    "        object_logits, _ = model_object(image)\n",
    "\n",
    "        predictions_object = post_processing(object_logits,\n",
    "                                             opt.image_size,\n",
    "                                             ObjectCLS,\n",
    "                                             model_object.detector.anchors,\n",
    "                                             opt.conf_threshold,\n",
    "                                             opt.nms_threshold)\n",
    "    else:\n",
    "        object_logits = None\n",
    "        predictions_object = None\n",
    "        \n",
    "    # relation\n",
    "    if np.array(obj_label).size > 0 and np.array(label).size > 0:\n",
    "        r_preds, r_obj_preds, relation_predictions = model_relation(image, label, obj_label)    \n",
    "    else:\n",
    "        r_preds = None\n",
    "        r_obj_preds = None\n",
    "        \n",
    "    print(\"===r_preds:{}\".format(r_preds))\n",
    "\n",
    "\n",
    "    # place\n",
    "    images_norm = []; info_place = []; preds_place = []\n",
    "    for idx in range(len(image)):\n",
    "        image_resize = image[idx]\n",
    "        images_norm.append(image_resize)\n",
    "        info_place.append(info[0][idx]['place'])\n",
    "        frame_place = frame_id.copy()\n",
    "    info_place = label_mapping(info_place)\n",
    "    buffer_images = place_buffer(images_norm, buffer_images)\n",
    "    pl_updated=False\n",
    "    buffer_idx = 10 - (len(images_norm) %10)\n",
    "    images_norm = buffer_images[-buffer_idx:] + images_norm\n",
    "    for plidx in range(len(images_norm)//10):\n",
    "        batch_images = torch.stack(images_norm[plidx*10:(plidx+1)*10]).cuda(device).unsqueeze(0)\n",
    "        output = model_place(batch_images)\n",
    "        output = torch.cat((output[:, :9], output[:, 10:]), 1) # None excluded. For None prediction, comment this line out.\n",
    "        preds = torch.argmax(output, -1).tolist() # (T, n_class) ->(T, )\n",
    "        for idx in range(len(preds)):\n",
    "            if preds[idx] >= 9: preds[idx] += 1\n",
    "        preds_place += preds;\n",
    "        pl_updated = True\n",
    "    buffer_images = images_norm[-10:]\n",
    "    preds_place = preds_place[buffer_idx:]\n",
    "    assert len(preds_place) == len(info_place)\n",
    "    preds_place_txt = label_remapping(preds_place)\n",
    "    target_place_txt = label_remapping(info_place)\n",
    "    \n",
    "    print(\"===preds_place_txt:{}\".format(preds_place_txt))\n",
    "    \n",
    "    for idx, frame in enumerate(frame_id):\n",
    "        \n",
    "        # ---------------(3) mkdir for evaluations----------------------\n",
    "        f_info = frame[0].split('/')\n",
    "        save_dir = '../results/drama-graph/{}/{}/{}/'.format(\n",
    "            f_info[4], f_info[5], f_info[6])\n",
    "        \n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        f_file = f_info[7]\n",
    "        mAP_file = \"{}_{}_{}_{}\".format(f_info[4],\n",
    "                                        f_info[5],\n",
    "                                        f_info[6],\n",
    "                                        f_info[7].replace(\"jpg\", \"txt\"))\n",
    "        if opt.display:\n",
    "            # AnotherMissOh07_002_0036_IMAGE_0000002672.txt\n",
    "            print(\"frame.__len__{}, mAP_file:{}\".format(len(frame_id), mAP_file))\n",
    "            \n",
    "        # --------------(5) visualization of inferences ----------\n",
    "        # out of try : pdb.set_trace = lambda : None\n",
    "        #**************************************\n",
    "        graph_json = {}\n",
    "        graph_json['persons'] = {}\n",
    "        graph_json['objects'] = {}\n",
    "        graph_json['relations'] = {}\n",
    "        graph_json['sound'] = 'none'\n",
    "        graph_json['place'] = 'none'\n",
    "        #**************************************\n",
    "        frm_name = \"episode_{:02d}_{}_{}_{}\".format(episode,\n",
    "                                                    f_info[5],\n",
    "                                                    f_info[6],\n",
    "                                                    f_info[7].replace(\".jpg\",\"\"))\n",
    "        save_file = save_dir + frm_name\n",
    "        \n",
    "        shot = int(f_info[6])\n",
    "        scene = int(f_info[5])\n",
    "        episode = episode\n",
    "        \n",
    "        try:\n",
    "            # for some empty video clips\n",
    "            img = image[idx]\n",
    "            # ToTensor function normalizes image pixel values into [0,1]\n",
    "            np_img = img.cpu().numpy()\n",
    "            np_img = np.transpose(np_img,(1,2,0)) * 255\n",
    "            output_image = cv2.cvtColor(np_img,cv2.COLOR_RGB2BGR)\n",
    "            output_image = cv2.resize(output_image, (width, height))\n",
    "            \n",
    "            # face and emotion\n",
    "            if len(predictions_face) != 0:\n",
    "                print(\"===== predictions_face: {}\".format(predictions_face))\n",
    "                prediction_face = predictions_face[idx]\n",
    "                prediction_emo  = emo_logits[idx]\n",
    "                for pi,pred in enumerate(prediction_face):\n",
    "                    xmin = int(max(pred[0] / width_ratio, 0))\n",
    "                    ymin = int(max(pred[1] / height_ratio, 0))\n",
    "                    xmax = int(min((pred[2]) / width_ratio, width))\n",
    "                    ymax = int(min((pred[3]) / height_ratio, height))\n",
    "                    color = colors[FaceCLS.index(pred[5])]\n",
    "\n",
    "                    cv2.rectangle(output_image, (xmin, ymin),\n",
    "                                  (xmax, ymax), color, 2)\n",
    "                    text_size = cv2.getTextSize(\n",
    "                        pred[5] + ' : %.2f' % pred[4],\n",
    "                        cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n",
    "                    cv2.rectangle(\n",
    "                        output_image,\n",
    "                        (xmin, ymin),\n",
    "                        (xmin + text_size[0] + 100,\n",
    "                         ymin + text_size[1] + 20), color, -1)\n",
    "                    cv2.putText(\n",
    "                        output_image, pred[5] + ' : %.2f' % pred[4],\n",
    "                        (xmin, ymin + text_size[1] + 4),\n",
    "                        cv2.FONT_HERSHEY_PLAIN, 1,\n",
    "                        (255, 255, 255), 1)\n",
    "\n",
    "                    # save detection results\n",
    "                    pred_cls = pred[5]\n",
    "                    cat_pred = '%s %s %s %s %s %s\\n' % (\n",
    "                        pred_cls,\n",
    "                        str(pred[4]),\n",
    "                        str(xmin), str(ymin), str(xmax), str(ymax))\n",
    "\n",
    "                    print(\"face_pred:{}\".format(cat_pred))\n",
    "                    print(\"detected {}\".format(\n",
    "                        save_dir + \"{}\".format(f_file)))\n",
    "\n",
    "                    #**************************************************\n",
    "                    graph_json['persons'][pred_cls] = {}\n",
    "                    #**************************************************\n",
    "\n",
    "                    # update emotion model and the prediction\n",
    "                    emo_ij = F.softmax(prediction_emo[pi], dim=0).argmax().detach().cpu().numpy()\n",
    "                    emo_txt = EmoCLS[emo_ij]\n",
    "                    cv2.putText(output_image, emo_txt, (xmin, ymin),\n",
    "                                cv2.FONT_HERSHEY_PLAIN, 2, (0,255,255), 2,\n",
    "                                cv2.LINE_AA)\n",
    "                    #******************************************************\n",
    "                    graph_json['persons'][pred_cls]['emotion'] = emo_txt\n",
    "                    #******************************************************\n",
    "            else:\n",
    "                print(\"===== None-predictions_face: {}\".format(predictions_face))\n",
    "            \n",
    "\n",
    "            if len(predictions_p) != 0 :\n",
    "                print(\"===== predictions_p: {}\".format(predictions_p))\n",
    "                prediction = predictions_p[idx]\n",
    "                \n",
    "                if True:\n",
    "                    b_logit = b_logits[idx]\n",
    "\n",
    "                # person and behavior\n",
    "                num_preds = len(prediction)\n",
    "                                \n",
    "                for jdx, pred in enumerate(prediction):\n",
    "                    # person\n",
    "                    xmin = int(max(pred[0] / width_ratio, 0))\n",
    "                    ymin = int(max(pred[1] / height_ratio, 0))\n",
    "                    xmax = int(min((pred[2]) / width_ratio, width))\n",
    "                    ymax = int(min((pred[3]) / height_ratio, height))\n",
    "                    color = colors[PersonCLS.index(pred[5])]\n",
    "\n",
    "                    cv2.rectangle(output_image, (xmin, ymin),\n",
    "                                  (xmax, ymax), color, 2)\n",
    "                    \n",
    "                    text_size = cv2.getTextSize(\n",
    "                        pred[5] + ' : %.2f' % pred[4],\n",
    "                        cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n",
    "                    cv2.rectangle(\n",
    "                        output_image,\n",
    "                        (xmin, ymin),\n",
    "                        (xmin + text_size[0] + 100,\n",
    "                         ymin + text_size[1] + 20), color, -1)\n",
    "                    cv2.putText(\n",
    "                        output_image, pred[5] + ' : %.2f' % pred[4],\n",
    "                        (xmin, ymin + text_size[1] + 4),\n",
    "                        cv2.FONT_HERSHEY_PLAIN, 1,\n",
    "                        (255, 255, 255), 1)\n",
    "                        \n",
    "                    pred_cls = pred[5]\n",
    "                    cat_pred = '%s %s %s %s %s %s\\n' % (\n",
    "                        pred_cls,\n",
    "                        str(pred[4]),\n",
    "                        str(xmin), str(ymin), str(xmax), str(ymax))\n",
    "                    print(\"person_pred:{}\".format(cat_pred))\n",
    "                    \n",
    "                    #**************************************************\n",
    "                    if pred_cls not in graph_json['persons'].keys():\n",
    "                        graph_json['persons'][pred_cls] = {}\n",
    "                    #**************************************************\n",
    "                    \n",
    "                    # behavior\n",
    "                    if True:\n",
    "                        value, index = b_logit[jdx].max(0)\n",
    "\n",
    "                        b_idx = index.cpu().numpy()\n",
    "                        b_pred = PBeHavCLS[b_idx]\n",
    "                        \n",
    "                        cv2.putText(\n",
    "                            output_image, '+ behavior : ' + b_pred,\n",
    "                            (xmin, ymin + text_size[1] + 4 + 12),\n",
    "                            cv2.FONT_HERSHEY_PLAIN, 1,\n",
    "                            (255, 255, 255), 1)\n",
    "                        pred_beh_cls = b_pred.replace(' ', '_')\n",
    "                        pred_beh_cls = pred_beh_cls.replace('/', '_')\n",
    "                        \n",
    "                        #******************************************************\n",
    "                        graph_json['persons'][pred_cls]['behavior'] = pred_beh_cls\n",
    "                        #******************************************************\n",
    "                        \n",
    "                        cat_pred_beh = '%s %s %s %s %s %s\\n' % (\n",
    "                            pred_beh_cls,\n",
    "                            str(pred[4]),\n",
    "                            str(xmin), str(ymin), str(xmax), str(ymax))\n",
    "\n",
    "                        print(\"behavior_pred:{}\".format(cat_pred_beh))\n",
    "            else:\n",
    "                print(\"===== None-predictions_p: {}\".format(predictions_p))\n",
    "\n",
    "            # object\n",
    "            if predictions_object is not None and len(predictions_object) > 0:\n",
    "                print(\"===== predictions_object: {}\".format(predictions_object))\n",
    "                import ipdb; ipdb.set_trace()\n",
    "                prediction_object = predictions_object[0]\n",
    "                num_preds = len(prediction)\n",
    "                for jdx, pred in enumerate(prediction_object):\n",
    "                    \n",
    "                    # save detection results\n",
    "                    pred_obj_cls = pred[5]\n",
    "                    \n",
    "                    #if True:\n",
    "                    xmin = int(max(pred[0] / width_ratio, 0))\n",
    "                    ymin = int(max(pred[1] / height_ratio, 0))\n",
    "                    xmax = int(min((pred[2]) / width_ratio, width))\n",
    "                    ymax = int(min((pred[3]) / height_ratio, height))\n",
    "                    color = colors[ObjectCLS.index(pred[5])]\n",
    "\n",
    "                    cv2.rectangle(output_image, (xmin, ymin),\n",
    "                                  (xmax, ymax), color, 2)\n",
    "\n",
    "                    text_size = cv2.getTextSize(\n",
    "                        pred[5] + ' : %.2f' % pred[4],\n",
    "                        cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n",
    "                    cv2.rectangle(\n",
    "                        output_image,\n",
    "                        (xmin, ymin),\n",
    "                        (xmin + text_size[0] + 100,\n",
    "                         ymin + text_size[1] + 20), color, -1)\n",
    "                    cv2.putText(\n",
    "                        output_image, pred[5] + ' : %.2f' % pred[4],\n",
    "                        (xmin, ymin + text_size[1] + 4),\n",
    "                        cv2.FONT_HERSHEY_PLAIN, 1,\n",
    "                        (255, 255, 255), 1)\n",
    "\n",
    "                    cat_pred = '%s %s %s %s %s %s\\n' % (\n",
    "                        pred_obj_cls,\n",
    "                        str(pred[4]),\n",
    "                        str(xmin), str(ymin), str(xmax), str(ymax))\n",
    "\n",
    "                    print(\"object_pred:{}\".format(cat_pred))\n",
    "                    \n",
    "                    #**************************************************\n",
    "                    graph_json['objects'][pred_obj_cls] = {}\n",
    "                    #**************************************************\n",
    "                    \n",
    "            else:\n",
    "                print(\"===== None-predictions_object: {}\".format(predictions_object))\n",
    "                        \n",
    "            # relation\n",
    "            if r_preds is not None and len(r_preds):\n",
    "                r_pred = r_preds[idx]\n",
    "                r_obj_pred = r_obj_preds[idx]\n",
    "                relation_prediction = relation_predictions[idx]\n",
    "                num_preds = len(r_pred)\n",
    "                \n",
    "                print(\"===== r_preds: {}\".format(r_preds))\n",
    "                for jdx, pred in enumerate(r_pred):\n",
    "                    \n",
    "                    pred_per_cls = pred[5]\n",
    "                    #if False:\n",
    "                    xmin = int(max(float(pred[0]) / width_ratio, 0))\n",
    "                    ymin = int(max(float(pred[1]) / height_ratio, 0))\n",
    "                    xmax = int(min((float(pred[2])) / width_ratio, width))\n",
    "                    ymax = int(min((float(pred[3])) / height_ratio, height))\n",
    "                    color = colors[PersonCLS.index(pred[5])]\n",
    "\n",
    "                    cv2.rectangle(output_image, (xmin, ymin),\n",
    "                                  (xmax, ymax), color, 2)\n",
    "\n",
    "                    text_size = cv2.getTextSize(\n",
    "                        pred_per_cls + ' : %.2f' % float(pred[4]),\n",
    "                        cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n",
    "                    cv2.rectangle(\n",
    "                        output_image,\n",
    "                        (xmin, ymin),\n",
    "                        (xmin + text_size[0] + 100,\n",
    "                         ymin + text_size[1] + 20), color, -1)\n",
    "                    cv2.putText(\n",
    "                        output_image, pred[5] + ' : %.2f' % float(pred[4]),\n",
    "                        (xmin, ymin + text_size[1] + 4),\n",
    "                        cv2.FONT_HERSHEY_PLAIN, 1,\n",
    "                        (255, 255, 255), 1)\n",
    "                    \n",
    "                    #*****************************************************\n",
    "                    graph_json['relations'][pred_per_cls] = {}\n",
    "                    #*****************************************************\n",
    "\n",
    "                    for kdx, obj_pred in enumerate(r_obj_pred):\n",
    "                        pred_obj_cls = obj_pred[5]\n",
    "                        \n",
    "                        #if False:\n",
    "                        xmin = int(max(float(obj_pred[0]) / width_ratio, 0))\n",
    "                        ymin = int(max(float(obj_pred[1]) / height_ratio, 0))\n",
    "                        xmax = int(min((float(obj_pred[2])) / width_ratio, width))\n",
    "                        ymax = int(min((float(obj_pred[3])) / height_ratio, height))\n",
    "\n",
    "                        color = colors[ObjectCLS.index(obj_pred[5])]\n",
    "                        cv2.rectangle(output_image, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "\n",
    "                        text_size = cv2.getTextSize(\n",
    "                            pred_obj_cls + ' : %.2f' % float(obj_pred[4]),\n",
    "                            cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n",
    "                        cv2.rectangle(\n",
    "                            output_image,\n",
    "                            (xmin, ymin),\n",
    "                            (xmin + text_size[0] + 100,\n",
    "                             ymin + text_size[1] + 20), color, -1)\n",
    "                        cv2.putText(\n",
    "                            output_image, obj_pred[5] + ' : %.2f' % float(obj_pred[4]),\n",
    "                            (xmin, ymin + text_size[1] + 4),\n",
    "                            cv2.FONT_HERSHEY_PLAIN, 1,\n",
    "                            (255, 255, 255), 1)\n",
    "                        \n",
    "                        value, ind = relation_prediction[kdx].max(1)\n",
    "                        ind = int(ind.cpu().numpy())\n",
    "                        rel_ind = P2ORelCLS[ind]\n",
    "                        pred_pred_cls = rel_ind\n",
    "                        \n",
    "                        #if False:\n",
    "                        cv2.putText(\n",
    "                            output_image, '+ relation : ' + rel_ind,\n",
    "                            (xmin, ymin + text_size[1] + 4 + 12),\n",
    "                            cv2.FONT_HERSHEY_PLAIN, 1,\n",
    "                            (255, 255, 255), 1)                        \n",
    "\n",
    "                        cat_pred = '%s %s %s %s %s\\n' % (\n",
    "                            pred_cls, str(xmin), str(ymin), str(xmax), str(ymax))\n",
    "                        print(\"relation_pred:{}\".format(cat_pred))\n",
    "                        \n",
    "                        #*****************************************************\n",
    "                        graph_json['relations'][pred_per_cls][pred_obj_cls] = pred_pred_cls\n",
    "                        #*****************************************************\n",
    "            else:\n",
    "                print(\"===== None-r_preds: {}\".format(r_preds))\n",
    "                        \n",
    "            # place\n",
    "            if len(preds_place_txt) != 0:\n",
    "                print(\"===== place_pred: {}\".format(preds_place_txt))\n",
    "                cv2.putText(output_image, \"place : \" + preds_place_txt[idx],\n",
    "                    (30, 30),\n",
    "                    cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 255), 2)\n",
    "                \n",
    "                #*****************************************\n",
    "                graph_json['place'] = preds_place_txt[idx]\n",
    "                #*****************************************\n",
    "                print('place_pred :', preds_place_txt[idx])\n",
    "                \n",
    "            else:\n",
    "                print(\"===== None-place_pred: {}\".format(preds_place_txt))\n",
    "            \n",
    "            # save output image  \n",
    "            cv2.imwrite(save_dir + \"{}\".format(f_file), output_image)\n",
    "            # save images\n",
    "            plt_output_image = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)\n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.imshow(plt_output_image.astype('uint8'))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            dot_to_json = json.dumps(graph_json)\n",
    "            with open('{}.json'.format(save_file), 'w') as f:\n",
    "                json.dump(dot_to_json, f)\n",
    "                print(graph_json)\n",
    "            graph_to_json(episode, scene, shot, graph_json, save_file)\n",
    "        except:\n",
    "            \n",
    "            # save output image  \n",
    "            cv2.imwrite(save_dir + \"{}\".format(f_file), output_image)\n",
    "            # save images\n",
    "            plt_output_image = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)\n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.imshow(plt_output_image.astype('uint8'))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            dot_to_json = json.dumps(graph_json)\n",
    "            with open('{}.json'.format(save_file), 'w') as f:\n",
    "                json.dump(dot_to_json, f)\n",
    "                print(graph_json)\n",
    "            graph_to_json(episode, scene, shot, graph_json, save_file)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
