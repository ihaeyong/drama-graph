{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MissOh DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AnotherMissOh Visual Structure\n",
    "- json_data['file_name'] : 'AnotherMissOh01.mp4'\n",
    "- json_data['visual_results']\n",
    "- json_data['visual_results'][0].keys() : dict_keys(['start_time', 'end_time', 'vid', 'image_info'])\n",
    "- {\n",
    "'start_time': '00:02:51;16', \n",
    "'end_time': '00:02:54;15', \n",
    "'vid': 'AnotherMissOh01_001_0078', \n",
    "'image_info': ...}\n",
    "- json_data['visual_results'][0]['image_info']\n",
    "- [{'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004295', \n",
    "'place': 'none', \n",
    "'persons': [\n",
    "{'person_id': 'Haeyoung1', \n",
    "'person_info': {\n",
    "'face_rect': {'min_x': 515, 'min_y': 0, 'max_x': 845, 'max_y': 443}, \n",
    "'full_rect': {'min_x': 278, 'min_y': 2, 'max_x': 1025, 'max_y': 769}, \n",
    "'behavior': 'stand up', \n",
    "'predicate': 'none', \n",
    "'emotion': 'Neutral', \n",
    "'face_rect_score': '0.5', \n",
    "'full_rect_score': '0.9'}, \n",
    "'related_objects': []}], \n",
    "'objects': []}, \n",
    "- {'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004311', \n",
    "'place': '', \n",
    "'persons': [{\n",
    "'person_id':'Haeyoung1',\n",
    "'person_info': {\n",
    "'face_rect': {'min_x': 515, 'min_y': 0, 'max_x': 831, 'max_y': 411}, \n",
    "'full_rect': {'min_x': 270, 'min_y': 0, 'max_x': 1025, 'max_y': 768}, \n",
    "'behavior': 'stand up', \n",
    "'predicate': 'none', \n",
    "'emotion': 'Neutral', \n",
    "'face_rect_score': '0.5', \n",
    "'full_rect_score': '0.9'}, \n",
    "'related_objects': []}],\n",
    "'objects': []},]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get install graphviz xdg-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"../\") # go to parent dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import json\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Yolo_v2_pytorch.src.utils import *\n",
    "from graphviz import Digraph, Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_blank(s):\n",
    "    return bool(s and s.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person\n",
      "(39, 129, 113)\n"
     ]
    }
   ],
   "source": [
    "MissOh_CLASSES = ['person']\n",
    "print(MissOh_CLASSES[0])\n",
    "global colors\n",
    "colors = pickle.load(open(\"../Yolo_v2_pytorch/src/pallete\", \"rb\"))\n",
    "print(colors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"../\") # go to parent dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, conf_threshold=0.35, data_path_test='./Yolo_v2_pytorch/missoh_test/', display=False, emo_net_ch=64, image_size=448, img_path='./data/AnotherMissOh/AnotherMissOh_images_ver3.2/', json_path='./data/AnotherMissOh/AnotherMissOh_Visual_ver3.2/', model='baseline', nms_threshold=0.5, pre_trained_model_type='model', saved_path='./checkpoint/refined_models')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "from Yolo_v2_pytorch.src.utils import *\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from Yolo_v2_pytorch.src.yolo_net import Yolo\n",
    "from Yolo_v2_pytorch.src.anotherMissOh_dataset import AnotherMissOh, Splits, SortFullRect, PersonCLS,PBeHavCLS, FaceCLS, ObjectCLS, P2ORelCLS\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from lib.place_model import place_model, label_mapping, accuracy, label_remapping, place_buffer\n",
    "from lib.person_model import person_model\n",
    "from lib.behavior_model import behavior_model\n",
    "from lib.pytorch_misc import optimistic_restore, de_chunkize, clip_grad_norm, flatten\n",
    "from lib.focal_loss import FocalLossWithOneHot, FocalLossWithOutOneHot, CELossWithOutOneHot\n",
    "from lib.face_model import face_model\n",
    "from lib.object_model import object_model\n",
    "from lib.relation_model import relation_model\n",
    "from lib.emotion_model import emotion_model, crop_face_emotion, EmoCLS\n",
    "\n",
    "num_persons = len(PersonCLS)\n",
    "num_behaviors = len(PBeHavCLS)\n",
    "num_faces = len(FaceCLS)\n",
    "num_objects = len(ObjectCLS)\n",
    "num_relations = len(P2ORelCLS)\n",
    "num_emos = len(EmoCLS)\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        \"You Only Look Once: Unified, Real-Time Object Detection\")\n",
    "    parser.add_argument(\"--image_size\",\n",
    "                        type=int, default=448,\n",
    "                        help=\"The common width and height for all images\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1,\n",
    "                        help=\"The number of images per batch\")\n",
    "    parser.add_argument(\"--conf_threshold\",\n",
    "                        type=float, default=0.35)\n",
    "    parser.add_argument(\"--nms_threshold\",\n",
    "                        type=float, default=0.5)\n",
    "    parser.add_argument(\"--pre_trained_model_type\",\n",
    "                        type=str, choices=[\"model\", \"params\"],\n",
    "                        default=\"model\")\n",
    "    parser.add_argument(\"--data_path_test\",\n",
    "                        type=str,\n",
    "                        default=\"./Yolo_v2_pytorch/missoh_test/\",\n",
    "                        help=\"the root folder of dataset\")\n",
    "\n",
    "    parser.add_argument(\"--saved_path\", type=str,\n",
    "                        default=\"./checkpoint/refined_models\")\n",
    "\n",
    "    parser.add_argument(\"--img_path\", type=str,\n",
    "                        default=\"./data/AnotherMissOh/AnotherMissOh_images_ver3.2/\")\n",
    "    parser.add_argument(\"--json_path\", type=str,\n",
    "                        default=\"./data/AnotherMissOh/AnotherMissOh_Visual_ver3.2/\")\n",
    "    parser.add_argument(\"-model\", dest='model', type=str, default=\"baseline\")\n",
    "    parser.add_argument(\"-display\", dest='display', action='store_true')\n",
    "    parser.add_argument(\"-emo_net_ch\", dest='emo_net_ch',type=int, default=64)\n",
    "    args = parser.parse_args([])\n",
    "    return args\n",
    "\n",
    "# get args.\n",
    "opt = get_args()\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.drawing.nx_pydot import read_dot\n",
    "#from networkx.drawing.nx_agraph import read_dot\n",
    "from networkx.readwrite import json_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.img_path = \"../data/AnotherMissOh/AnotherMissOh_images_ver3.2/\"\n",
    "opt.json_path = \"../data/AnotherMissOh/AnotherMissOh_Visual_ver3.2/\"\n",
    "opt.saved_path = \"../checkpoint/refined_models\"\n",
    "opt.display = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c30636170066>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# load datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAnotherMissOh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAnotherMissOh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAnotherMissOh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/drama-graph/Yolo_v2_pytorch/src/anotherMissOh_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, img_path, json_path, display_log)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisplay_log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_clips\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_clips\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/drama-graph/Yolo_v2_pytorch/src/anotherMissOh_dataset.py\u001b[0m in \u001b[0;36mload_json\u001b[0;34m(self, dataset, img_path, json_path)\u001b[0m\n\u001b[1;32m    391\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------{}th person-----------\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m                         \u001b[0mimage_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'persons'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'person_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'person_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'related_objects'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tform = [\n",
    "    Resize((448, 448)),  # should match to Yolo_V2\n",
    "    ToTensor(),\n",
    "    # Normalize(# should match to Yolo_V2\n",
    "    #mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "]\n",
    "transf = Compose(tform)\n",
    "\n",
    "# splits the episodes int train, val, test\n",
    "train, val, test = Splits(num_episodes=18)\n",
    "\n",
    "# load datasets\n",
    "train_set = AnotherMissOh(train, opt.img_path, opt.json_path, False)\n",
    "val_set = AnotherMissOh(val, opt.img_path, opt.json_path, False)\n",
    "test_set = AnotherMissOh(test, opt.img_path, opt.json_path, False)\n",
    "\n",
    "episode = 7\n",
    "infer = [episode]\n",
    "infer_set = AnotherMissOh(infer, opt.img_path, opt.json_path, False)\n",
    "\n",
    "\n",
    "# model path\n",
    "model_path = \"{}/anotherMissOh_{}.pth\".format(\n",
    "    opt.saved_path,opt.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(123)\n",
    "    device = torch.cuda.current_device()\n",
    "else:\n",
    "    torch.manual_seed(123)\n",
    "\n",
    "# set test loader params\n",
    "test_params = {\"batch_size\": opt.batch_size,\n",
    "               \"shuffle\": False,\n",
    "               \"drop_last\": False,\n",
    "               \"collate_fn\": custom_collate_fn}\n",
    "\n",
    "# set test loader\n",
    "test_loader = DataLoader(infer_set, **test_params)\n",
    "\n",
    "# ---------------(1) load refined models --------------------\n",
    "# get the trained models from\n",
    "# https://drive.google.com/drive/folders/1WXzP8nfXU4l0cNOtSPX9O1qxYH2m6LIp\n",
    "# define person model\n",
    "if True:\n",
    "    # model path\n",
    "    if False:\n",
    "        model_path = \"../checkpoint/person/anotherMissOh_only_params_{}\".format(\n",
    "            'voc_person_group_1gpu_init_none.pth')\n",
    "    else:\n",
    "        model_path = \"../checkpoint/person/anotherMissOh_{}\".format(\n",
    "            'voc_person_group_1gpu_init_none.pth')\n",
    "        \n",
    "    model_p = person_model(num_persons, device)\n",
    "    ckpt = torch.load(model_path)\n",
    "    \n",
    "    # in case of multi-gpu training\n",
    "    if False:\n",
    "        from collections import OrderedDict\n",
    "        ckpt_state_dict = OrderedDict()\n",
    "        for k,v in ckpt.items():\n",
    "            name = k[7:] # remove 'module'\n",
    "            ckpt_state_dict[name] = v\n",
    "\n",
    "        print(\"--- loading {} model---\".format(model_path))\n",
    "        if optimistic_restore(model_p, ckpt_state_dict):\n",
    "            print(\"loaded with {}\".format(model_path))\n",
    "    else:\n",
    "        model_p = ckpt\n",
    "        print(\"loaded with {}\".format(model_path))\n",
    "        \n",
    "    model_p.to(device)\n",
    "    model_p.eval()\n",
    "\n",
    "if False :\n",
    "    print(\"-----------person---behavior-------model---------------\")\n",
    "    model1 = behavior_model(num_persons, num_behaviors, opt, device)\n",
    "    trained_persons = '../checkpoint/refined_models' + os.sep + \"{}\".format(\n",
    "    'anotherMissOh_only_params_integration.pth')\n",
    "    if optimistic_restore(model1, torch.load(trained_persons)):\n",
    "        #model1.load_state_dict(torch.load(trained_persons))\n",
    "        print(\"loaded with {}\".format(trained_persons))\n",
    "\n",
    "else:\n",
    "    # pre-trained behavior model\n",
    "    # step 1: person trained on voc 50 epoch\n",
    "    # step 2: person feature based behavior sequence learning 100 epoch\n",
    "    model1 = behavior_model(num_persons, num_behaviors, opt, device)\n",
    "    if False:\n",
    "        trained_persons = '../checkpoint/refined_models' + os.sep + \"{}\".format(\n",
    "            'anotherMissOh_only_params_integration.pth')\n",
    "        model1.load_state_dict(torch.load(trained_persons))\n",
    "        print(\"loaded with person and behavior model {}\".format(trained_persons))\n",
    "    else:\n",
    "        trained_persons = '../checkpoint/behavior' + os.sep + \"{}\".format(\n",
    "            'anotherMissOh_global_diff_subset_batch1_local_wloss_output_1_noise_global.pth')\n",
    "        model1 = torch.load(trained_persons)\n",
    "        print(\"loaded with person and behavior model {}\".format(trained_persons))\n",
    "        \n",
    "model1.cuda(device)\n",
    "model1.eval()\n",
    "\n",
    "# face model\n",
    "if True:\n",
    "    model_face = face_model(num_persons, num_faces, device)\n",
    "    trained_face = '../checkpoint/refined_models' + os.sep + \"{}\".format(\n",
    "    'anotherMissOh_only_params_face.pth')\n",
    "    model_face.load_state_dict(torch.load(trained_face))\n",
    "    print(\"loaded with {}\".format(trained_face))\n",
    "model_face.cuda(device)\n",
    "model_face.eval()\n",
    "\n",
    "# emotion model\n",
    "if True:\n",
    "    model_emo = emotion_model(opt.emo_net_ch, num_persons, device)\n",
    "    trained_emotion = '../checkpoint/refined_models' + os.sep + \"{}\".format(\n",
    "    'anotherMissOh_only_params_emotion_integration.pth')\n",
    "    model_emo.load_state_dict(torch.load(trained_emotion))\n",
    "    print(\"loaded with {}\".format(trained_emotion))\n",
    "model_emo.cuda(device)\n",
    "model_emo.eval()\n",
    "\n",
    "# object model\n",
    "if True:\n",
    "    # add model\n",
    "    model_object = object_model(num_objects)\n",
    "    trained_object = '../checkpoint/refined_models' + os.sep + \"{}\".format(\n",
    "    'anotherMissOh_only_params_object_integration.pth')\n",
    "    # model load\n",
    "    print(\"loaded with {}\".format(trained_object))\n",
    "    model_object.load_state_dict(torch.load(trained_object))\n",
    "\n",
    "model_object.cuda(device)\n",
    "model_object.eval()\n",
    "\n",
    "\n",
    "# relation model\n",
    "if True:\n",
    "    # add model\n",
    "    model_relation = relation_model(num_persons, num_objects, num_relations, opt, device)\n",
    "    trained_relation = '../checkpoint/refined_models' + os.sep + \"{}\".format(\n",
    "    'anotherMissOh_only_params_relation_integration.pth')\n",
    "    # model load\n",
    "    print(\"loaded with {}\".format(trained_relation))\n",
    "    model_relation.load_state_dict(torch.load(trained_relation))\n",
    "model_relation.cuda(device)\n",
    "model_relation.eval()\n",
    "\n",
    "# place model\n",
    "if True:\n",
    "    model_place = place_model(num_persons, num_behaviors, device)\n",
    "    # add model\n",
    "    trained_place = '../checkpoint/refined_models' + os.sep + \"{}\".format(\n",
    "        'anotherMissOh_only_params_place_integration.pth')\n",
    "    # model load\n",
    "    print(\"loaded with {}\".format(trained_place))\n",
    "    model_place.load_state_dict(torch.load(trained_place)['model'])\n",
    "model_place.cuda(device)\n",
    "model_place.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the color map for detection results\n",
    "colors = pickle.load(open(\"../Yolo_v2_pytorch/src/pallete\", \"rb\"))\n",
    "\n",
    "width, height = (1024, 768)\n",
    "width_ratio = float(opt.image_size) / width\n",
    "height_ratio = float(opt.image_size) / height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_blank(s):\n",
    "    return bool(s and s.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(episode, scene, frm, info, save_file=None):\n",
    "    \n",
    "    if save_file is None:\n",
    "        save_file = 'temp_graph'\n",
    "    import string\n",
    "    strseq = string.ascii_uppercase\n",
    "    \n",
    "    # define  graph\n",
    "    dot = Digraph('G',filename='{}.gv'.format(save_file),engine='fdp')\n",
    "    dot.attr('graph', rotate = '0', dpi='600',rankdir='TB', size='10,8')\n",
    "    dot.attr('node', height='0.1', fontsize='6')\n",
    "    dot.attr('edge', fontsize='6')\n",
    "\n",
    "    place = \"{}\".format(info['place'])\n",
    "    sound = \"{}\".format('sound')\n",
    "    \n",
    "    if not is_not_blank(place):\n",
    "        place = 'none'\n",
    "    if not is_not_blank(sound):\n",
    "        sound = 'none'\n",
    "        \n",
    "    num_of_persons = len(info['persons']['person_id'])\n",
    "    num_of_objects = len(info['objects']['object_id'])\n",
    "    \n",
    "    frm_graph = 'episode_{}_scene_{}_frame_{}'.format(\n",
    "        episode, scene, frm)\n",
    "    \n",
    "    #dot.node(frm_graph, style='filled', color='lightgrey')\n",
    "    episode_node = \"episode_{:02d}\".format(episode)\n",
    "    scene_node = \"scene_{:03d}\".format(scene)\n",
    "    frame_node = \"frame_{:04d}\".format(frm)\n",
    "    dot.node(episode_node, style='filled', color='lightgrey')\n",
    "    dot.node(scene_node, style='filled', color='lightgrey')\n",
    "    dot.node(frame_node, style='filled', color='lightgrey')\n",
    "    \n",
    "    dot.node(place, style='filled', color='lightblue')\n",
    "    dot.node(sound, style='filled', color='lightblue')\n",
    "    \n",
    "    if is_not_blank(episode_node) and is_not_blank(scene_node):\n",
    "        dot.edge(episode_node, scene_node)\n",
    "    \n",
    "    if is_not_blank(scene_node) and is_not_blank(frame_node):\n",
    "        dot.edge(scene_node, frame_node)\n",
    "        \n",
    "    if is_not_blank(frame_node) and is_not_blank(place):\n",
    "        dot.edge(frame_node, place)\n",
    "    \n",
    "    if is_not_blank(frame_node) and is_not_blank(sound):\n",
    "        dot.edge(frame_node, sound)\n",
    "    \n",
    "    for p in range(num_of_objects):\n",
    "        try: \n",
    "            object_id = info['objects']['object_id'][p]\n",
    "        except:\n",
    "            object_id = 'none'\n",
    "            \n",
    "        try: \n",
    "            predicate = info['objects']['relation'][p]\n",
    "        except:\n",
    "            predicate = 'none'\n",
    "            \n",
    "        if is_not_blank(object_id) and object_id is not 'person':\n",
    "            dot.node(object_id, style='filled', color='gold')\n",
    "        if is_not_blank(predicate) and predicate is not 'person':\n",
    "            dot.node(predicate, style='filled', color='gold')\n",
    "        if is_not_blank(frame_node) and is_not_blank(object_id):\n",
    "            dot.edge(frame_node, object_id)\n",
    "            \n",
    "    for p in range(num_of_persons):\n",
    "        \n",
    "        try:\n",
    "            person_id = \"{}\".format(info['persons']['person_id'][p])\n",
    "        except:\n",
    "            person_id = 'none'\n",
    "        try:\n",
    "            behavior = \"{}\".format(info['persons']['behavior'][p])\n",
    "        except:\n",
    "            person_id = 'none'\n",
    "        try:\n",
    "            predicate = \"{}\".format(info['persons']['predicate'][p])\n",
    "        except:\n",
    "            person_id = 'none'\n",
    "        try:\n",
    "            emotion = \"{}\".format(info['persons']['emotion'][p])\n",
    "        except:\n",
    "            person_id = 'none'\n",
    "        try:\n",
    "            robj_id = \"{}\".format(info['persons']['related_object_id'][p])\n",
    "        except:\n",
    "            robj_id = ''\n",
    "            \n",
    "        if is_not_blank(person_id):\n",
    "            dot.node(person_id)\n",
    "        if is_not_blank(behavior):\n",
    "            dot.node(behavior, style='filled', color='green')\n",
    "        #if is_not_blank(predicate):\n",
    "        #    dot.node(predicate, style='filled', color='yellow')\n",
    "        if is_not_blank(emotion):\n",
    "            dot.node(emotion, style='filled', color='blue')\n",
    "\n",
    "        if is_not_blank(frame_node) and is_not_blank(person_id):\n",
    "            dot.edge(frame_node, person_id)\n",
    "        if is_not_blank(person_id) and is_not_blank(behavior):\n",
    "            dot.edge(person_id, behavior)\n",
    "        if is_not_blank(person_id) and is_not_blank(predicate) and is_not_blank(robj_id):\n",
    "            dot.edge(person_id, robj_id, label=predicate, color='red')\n",
    "            #dot.edge(predicate, robj_id)\n",
    "        if is_not_blank(person_id) and is_not_blank(emotion):\n",
    "            dot.edge(person_id, emotion)\n",
    "            \n",
    "\n",
    "    # convert dot graph to json\n",
    "    if False:\n",
    "        dot_to_json =json.dumps(json_graph.node_link_data(dot))\n",
    "    else:\n",
    "        dot_to_json = json.dumps(info)\n",
    "    \n",
    "    with open('{}.json'.format(save_file), 'w') as f:\n",
    "        json.dump(dot_to_json, f)\n",
    "        \n",
    "    # show in image\n",
    "    dot.format = 'png'\n",
    "    dot.render('{}.gv'.format(save_file), view=True) \n",
    "   \n",
    "    graph = cv2.imread('{}.gv.png'.format(save_file))\n",
    "    graph = cv2.resize(graph, dsize=(0, 0), fx=600.0/graph.shape[0], fy=600.0/graph.shape[0])\n",
    "     \n",
    "    if True:\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.imshow(graph)\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = 0\n",
    "frm = 0\n",
    "images, info = test_set[clip]\n",
    "\n",
    "print(\"----------------------{}-th----------------frame\".format(frm))\n",
    "graph(episode, clip, frm, info[frm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sequence buffers\n",
    "buffer_images = []\n",
    "graph_info = {}\n",
    "# load test clips\n",
    "for iter, batch in enumerate(test_loader):\n",
    "    image, info = batch\n",
    "\n",
    "    scene = iter\n",
    "    episode = episode\n",
    "    \n",
    "    # sort label info on fullrect\n",
    "    image, label, behavior_label, obj_label, face_label, emo_label, frame_id = SortFullRect(\n",
    "        image, info, is_train=False)\n",
    "\n",
    "    try :\n",
    "        image = torch.cat(image,0).cuda(device)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # -----------------(2) inference -------------------------\n",
    "    # person and behavior predictions\n",
    "    # person\n",
    "    # logits : [1, 125, 14, 14]\n",
    "    p_logits, _ = model_p(image)\n",
    "    predictions_p = post_processing(p_logits,\n",
    "                                    opt.image_size,\n",
    "                                    PersonCLS,\n",
    "                                    model_p.detector.anchors,\n",
    "                                    opt.conf_threshold,\n",
    "                                    opt.nms_threshold)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # logits : [1, 125, 14, 14]\n",
    "    # behavior_logits : [1, 135, 14, 14]\n",
    "    if False:\n",
    "        predictions, b_logits = model1(image, label, behavior_label)\n",
    "\n",
    "    # face\n",
    "    if np.array(face_label).size > 0 :\n",
    "        face_logits = model_face(image)\n",
    "        predictions_face = post_processing(face_logits,\n",
    "                                           opt.image_size,\n",
    "                                           FaceCLS,\n",
    "                                           model_face.detector.anchors,\n",
    "                                           opt.conf_threshold,\n",
    "                                           opt.nms_threshold)\n",
    "\n",
    "    # emotion\n",
    "    if np.array(face_label).size > 0 and False:\n",
    "        face_label = [fl for fl in face_label if len(fl) > 0]\n",
    "        emo_label = [el for el in emo_label if len(el) > 0]\n",
    "        image_c = image.permute(0,2,3,1).cpu()\n",
    "        face_crops, emo_gt = crop_face_emotion(image_c, face_label, emo_label, opt)\n",
    "        face_crops, emo_gt = face_crops.cuda(device).contiguous(), emo_gt.cuda(device)\n",
    "        emo_logits = model_emo(face_crops)\n",
    "        num_img, num_face = np.array(face_label).shape[0:2]\n",
    "        emo_logits = emo_logits.view(num_img, num_face, 7)\n",
    "\n",
    "    # object\n",
    "    if np.array(obj_label).size > 0 :\n",
    "        object_logits, _ = model_object(image)\n",
    "\n",
    "        predictions_object = post_processing(object_logits,\n",
    "                                             opt.image_size,\n",
    "                                             ObjectCLS,\n",
    "                                             model_object.detector.anchors,\n",
    "                                             opt.conf_threshold,\n",
    "                                             opt.nms_threshold)\n",
    "\n",
    "\n",
    "\n",
    "    # relation\n",
    "    if np.array(obj_label).size > 0 and np.array(label).size > 0:\n",
    "        r_preds, r_obj_preds, relation_predictions = model_relation(image, label, obj_label)\n",
    "\n",
    "\n",
    "    # place\n",
    "    images_norm = []; info_place = []; preds_place = []\n",
    "    for idx in range(len(image)):\n",
    "        image_resize = image[idx]\n",
    "        images_norm.append(image_resize)\n",
    "        info_place.append(info[0][idx]['place'])\n",
    "        frame_place = frame_id.copy()\n",
    "    info_place = label_mapping(info_place)\n",
    "    buffer_images = place_buffer(images_norm, buffer_images)\n",
    "    pl_updated=False\n",
    "    buffer_idx = 10 - (len(images_norm) %10)\n",
    "    images_norm = buffer_images[-buffer_idx:] + images_norm\n",
    "    for plidx in range(len(images_norm)//10):\n",
    "        batch_images = torch.stack(images_norm[plidx*10:(plidx+1)*10]).cuda(device).unsqueeze(0)\n",
    "        output = model_place(batch_images)\n",
    "        output = torch.cat((output[:, :9], output[:, 10:]), 1) # None excluded. For None prediction, comment this line out.\n",
    "        preds = torch.argmax(output, -1).tolist() # (T, n_class) ->(T, )\n",
    "        for idx in range(len(preds)):\n",
    "            if preds[idx] >= 9: preds[idx] += 1\n",
    "        preds_place += preds;\n",
    "        pl_updated = True\n",
    "    buffer_images = images_norm[-10:]\n",
    "    preds_place = preds_place[buffer_idx:]\n",
    "    assert len(preds_place) == len(info_place)\n",
    "    preds_place_txt = label_remapping(preds_place)\n",
    "    target_place_txt = label_remapping(info_place)\n",
    "    \n",
    "    for idx, frame in enumerate(frame_id):\n",
    "\n",
    "        # ---------------(3) mkdir for evaluations----------------------\n",
    "        f_info = frame[0].split('/')\n",
    "        save_dir = '../results/drama-graph/{}/{}/{}/'.format(\n",
    "            f_info[4], f_info[5], f_info[6])\n",
    "        \n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        f_file = f_info[7]\n",
    "        mAP_file = \"{}_{}_{}_{}\".format(f_info[4],\n",
    "                                        f_info[5],\n",
    "                                        f_info[6],\n",
    "                                        f_info[7].replace(\"jpg\", \"txt\"))\n",
    "        if opt.display:\n",
    "            print(\"frame.__len__{}, mAP_file:{}\".format(len(frame_id), mAP_file))\n",
    "            \n",
    "        # --------------(5) visualization of inferences ----------\n",
    "        # out of try : pdb.set_trace = lambda : None\n",
    "        try:\n",
    "            # for some empty video clips\n",
    "            img = image[idx]\n",
    "            # ToTensor function normalizes image pixel values into [0,1]\n",
    "            np_img = img.cpu().numpy()\n",
    "            np_img = np.transpose(np_img,(1,2,0)) * 255\n",
    "            output_image = cv2.cvtColor(np_img,cv2.COLOR_RGB2BGR)\n",
    "            output_image = cv2.resize(output_image, (width, height))\n",
    "            \n",
    "            #**************************************\n",
    "            graph_info['persons'] = {}\n",
    "            graph_info['persons']['person_id'] = []\n",
    "            graph_info['persons']['behavior'] = []\n",
    "            graph_info['persons']['emotion'] = []\n",
    "            #**************************************\n",
    "            graph_info['objects'] = {}\n",
    "            graph_info['objects']['object_id'] = []\n",
    "            graph_info['objects']['relation'] = []\n",
    "            #**************************************\n",
    "\n",
    "            if len(predictions_p) != 0 :\n",
    "                prediction = predictions_p[idx]\n",
    "                \n",
    "                if False:\n",
    "                    b_logit = b_logits[idx]\n",
    "\n",
    "                # person and behavior\n",
    "                num_preds = len(prediction)\n",
    "                                \n",
    "                for jdx, pred in enumerate(prediction):\n",
    "                    # person\n",
    "                    xmin = int(max(pred[0] / width_ratio, 0))\n",
    "                    ymin = int(max(pred[1] / height_ratio, 0))\n",
    "                    xmax = int(min((pred[2]) / width_ratio, width))\n",
    "                    ymax = int(min((pred[3]) / height_ratio, height))\n",
    "                    color = colors[PersonCLS.index(pred[5])]\n",
    "\n",
    "                    cv2.rectangle(output_image, (xmin, ymin),\n",
    "                                  (xmax, ymax), color, 2)\n",
    "                    \n",
    "                    text_size = cv2.getTextSize(\n",
    "                        pred[5] + ' : %.2f' % pred[4],\n",
    "                        cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n",
    "                    cv2.rectangle(\n",
    "                        output_image,\n",
    "                        (xmin, ymin),\n",
    "                        (xmin + text_size[0] + 100,\n",
    "                         ymin + text_size[1] + 20), color, -1)\n",
    "                    cv2.putText(\n",
    "                        output_image, pred[5] + ' : %.2f' % pred[4],\n",
    "                        (xmin, ymin + text_size[1] + 4),\n",
    "                        cv2.FONT_HERSHEY_PLAIN, 1,\n",
    "                        (255, 255, 255), 1)\n",
    "                        \n",
    "                    pred_cls = pred[5]\n",
    "                    cat_pred = '%s %s %s %s %s %s\\n' % (\n",
    "                        pred_cls,\n",
    "                        str(pred[4]),\n",
    "                        str(xmin), str(ymin), str(xmax), str(ymax))\n",
    "                    print(\"person_pred:{}\".format(cat_pred))\n",
    "                    \n",
    "                    #**************************************************\n",
    "                    graph_info['persons']['person_id'].append(pred_cls)\n",
    "                    #**************************************************\n",
    "                    \n",
    "                    # behavior\n",
    "                    if False:\n",
    "                        value, index = b_logit[jdx].max(0)\n",
    "\n",
    "                        b_idx = index.cpu().numpy()\n",
    "                        b_pred = PBeHavCLS[b_idx]\n",
    "                        \n",
    "                        cv2.putText(\n",
    "                            output_image, '+ behavior : ' + b_pred,\n",
    "                            (xmin, ymin + text_size[1] + 4 + 12),\n",
    "                            cv2.FONT_HERSHEY_PLAIN, 1,\n",
    "                            (255, 255, 255), 1)\n",
    "                        pred_beh_cls = b_pred.replace(' ', '_')\n",
    "                        pred_beh_cls = pred_beh_cls.replace('/', '_')\n",
    "                        \n",
    "                        #******************************************************\n",
    "                        graph_info['persons']['person_id'].append(pred_beh_cls)\n",
    "                        #******************************************************\n",
    "                        \n",
    "                        cat_pred_beh = '%s %s %s %s %s %s\\n' % (\n",
    "                            pred_beh_cls,\n",
    "                            str(pred[4]),\n",
    "                            str(xmin), str(ymin), str(xmax), str(ymax))\n",
    "\n",
    "                        print(\"behavior_pred:{}\".format(cat_pred_beh))\n",
    "                        \n",
    "                    # emotion\n",
    "                    if False:\n",
    "                        fl = face_label[idx][jdx]\n",
    "                        face_x0, face_y0 = int(fl[0]/width_ratio), int(fl[1]/height_ratio)\n",
    "                        face_x1, face_y1 = int(fl[2]/width_ratio), int(fl[3]/height_ratio)\n",
    "                        emo_ij = F.softmax(emo_logits[idx,jdx,:], dim=0).argmax().detach().cpu().numpy()\n",
    "                        emo_txt = EmoCLS[emo_ij]\n",
    "                        cv2.rectangle(output_image, (face_x0,face_y0),\n",
    "                                      (face_x1,face_y1), (255,255,0), 1)\n",
    "                        cv2.putText(output_image, emo_txt, (face_x0, face_y0-5),\n",
    "                                    cv2.FONT_HERSHEY_PLAIN, 1, (255,255,0), 1,\n",
    "                                    cv2.LINE_AA)\n",
    "                        \n",
    "                        #******************************************************\n",
    "                        graph_info['persons']['emotion'].append(emo_txt)\n",
    "                        #******************************************************\n",
    "                    \n",
    "                    if opt.display:\n",
    "                        print(\"detected {}\".format(save_dir + \"{}\".format(f_file)))\n",
    "                else:\n",
    "                    if opt.display:\n",
    "                        print(\"non-detected {}\".format(\n",
    "                        save_dir + \"{}\".format(f_file)))\n",
    "\n",
    "            # object\n",
    "            if len(predictions_object) != 0:\n",
    "                \n",
    "                prediction_object = predictions_object[0]\n",
    "                num_preds = len(prediction)\n",
    "                for jdx, pred in enumerate(prediction_object):\n",
    "                    xmin = int(max(pred[0] / width_ratio, 0))\n",
    "                    ymin = int(max(pred[1] / height_ratio, 0))\n",
    "                    xmax = int(min((pred[2]) / width_ratio, width))\n",
    "                    ymax = int(min((pred[3]) / height_ratio, height))\n",
    "                    color = colors[ObjectCLS.index(pred[5])]\n",
    "\n",
    "                    cv2.rectangle(output_image, (xmin, ymin),\n",
    "                                  (xmax, ymax), color, 2)\n",
    "\n",
    "                    text_size = cv2.getTextSize(\n",
    "                        pred[5] + ' : %.2f' % pred[4],\n",
    "                        cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n",
    "                    cv2.rectangle(\n",
    "                        output_image,\n",
    "                        (xmin, ymin),\n",
    "                        (xmin + text_size[0] + 100,\n",
    "                         ymin + text_size[1] + 20), color, -1)\n",
    "                    cv2.putText(\n",
    "                        output_image, pred[5] + ' : %.2f' % pred[4],\n",
    "                        (xmin, ymin + text_size[1] + 4),\n",
    "                        cv2.FONT_HERSHEY_PLAIN, 1,\n",
    "                        (255, 255, 255), 1)\n",
    "\n",
    "                    # save detection results\n",
    "                    pred_cls = pred[5]\n",
    "                    cat_pred = '%s %s %s %s %s %s\\n' % (\n",
    "                        pred_cls,\n",
    "                        str(pred[4]),\n",
    "                        str(xmin), str(ymin), str(xmax), str(ymax))\n",
    "                    \n",
    "                    #**************************************************\n",
    "                    graph_info['objects']['object_id'].append(pred_cls)\n",
    "                    #**************************************************\n",
    "                    print(\"object_pred:{}\".format(cat_pred))\n",
    "\n",
    "                    if opt.display:\n",
    "                        print(\"detected {}\".format(\n",
    "                            save_dir + \"{}\".format(f_file)))\n",
    "                else:\n",
    "                    if opt.display:\n",
    "                        print(\"non-detected {}\".format(\n",
    "                        save_dir + \"{}\".format(f_file)))\n",
    "                        \n",
    "            # relation\n",
    "            if len(r_preds) != 0:\n",
    "                r_pred = r_preds[idx]\n",
    "                r_obj_pred = r_obj_preds[idx]\n",
    "                relation_prediction = relation_predictions[idx]\n",
    "                num_preds = len(r_pred)\n",
    "                for jdx, pred in enumerate(r_pred):\n",
    "                    xmin = int(max(float(pred[0]) / width_ratio, 0))\n",
    "                    ymin = int(max(float(pred[1]) / height_ratio, 0))\n",
    "                    xmax = int(min((float(pred[2])) / width_ratio, width))\n",
    "                    ymax = int(min((float(pred[3])) / height_ratio, height))\n",
    "                    color = colors[PersonCLS.index(pred[5])]\n",
    "                    \n",
    "                    cv2.rectangle(output_image, (xmin, ymin),\n",
    "                                  (xmax, ymax), color, 2)\n",
    "\n",
    "                    text_size = cv2.getTextSize(\n",
    "                        pred[5] + ' : %.2f' % float(pred[4]),\n",
    "                        cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n",
    "                    cv2.rectangle(\n",
    "                        output_image,\n",
    "                        (xmin, ymin),\n",
    "                        (xmin + text_size[0] + 100,\n",
    "                         ymin + text_size[1] + 20), color, -1)\n",
    "                    cv2.putText(\n",
    "                        output_image, pred[5] + ' : %.2f' % float(pred[4]),\n",
    "                        (xmin, ymin + text_size[1] + 4),\n",
    "                        cv2.FONT_HERSHEY_PLAIN, 1,\n",
    "                        (255, 255, 255), 1)\n",
    "\n",
    "                    for kdx, obj_pred in enumerate(r_obj_pred):\n",
    "                        xmin = int(max(float(obj_pred[0]) / width_ratio, 0))\n",
    "                        ymin = int(max(float(obj_pred[1]) / height_ratio, 0))\n",
    "                        xmax = int(min((float(obj_pred[2])) / width_ratio, width))\n",
    "                        ymax = int(min((float(obj_pred[3])) / height_ratio, height))\n",
    "\n",
    "                        color = colors[ObjectCLS.index(obj_pred[5])]\n",
    "                        cv2.rectangle(output_image, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "                        \n",
    "                        text_size = cv2.getTextSize(\n",
    "                            obj_pred[5] + ' : %.2f' % float(obj_pred[4]),\n",
    "                            cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n",
    "                        cv2.rectangle(\n",
    "                            output_image,\n",
    "                            (xmin, ymin),\n",
    "                            (xmin + text_size[0] + 100,\n",
    "                             ymin + text_size[1] + 20), color, -1)\n",
    "                        cv2.putText(\n",
    "                            output_image, obj_pred[5] + ' : %.2f' % float(obj_pred[4]),\n",
    "                            (xmin, ymin + text_size[1] + 4),\n",
    "                            cv2.FONT_HERSHEY_PLAIN, 1,\n",
    "                            (255, 255, 255), 1)\n",
    "                        \n",
    "                        #*****************************************************\n",
    "                        graph_info['objects']['object_id'].append(obj_pred[5])\n",
    "                        #*****************************************************\n",
    "                        \n",
    "                        value, ind = relation_prediction[kdx].max(1)\n",
    "                        ind = int(ind.cpu().numpy())\n",
    "                        rel_ind = P2ORelCLS[ind]\n",
    "                        cv2.putText(\n",
    "                            output_image, '+ relation : ' + rel_ind,\n",
    "                            (xmin, ymin + text_size[1] + 4 + 12),\n",
    "                            cv2.FONT_HERSHEY_PLAIN, 1,\n",
    "                            (255, 255, 255), 1)\n",
    "\n",
    "                        pred_cls = rel_ind\n",
    "                        cat_pred = '%s %s %s %s %s\\n' % (\n",
    "                            pred_cls, str(xmin), str(ymin), str(xmax), str(ymax))\n",
    "                        print(\"relation_pred:{}\".format(cat_pred))\n",
    "                        \n",
    "                        #*****************************************************\n",
    "                        graph_info['objects']['relation'].append(pred_cls)\n",
    "                        #*****************************************************\n",
    "                        \n",
    "            # place\n",
    "            if len(preds_place_txt) != 0:\n",
    "                cv2.putText(output_image, \"place : \" + preds_place_txt[idx],\n",
    "                    (30, 30),\n",
    "                    cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 255), 2)\n",
    "                \n",
    "                #*****************************************\n",
    "                graph_info['place'] = preds_place_txt[idx]\n",
    "                #*****************************************\n",
    "                \n",
    "                if opt.display:\n",
    "                    print('place_pred :', preds_place_txt[idx])\n",
    "                    \n",
    "            # face\n",
    "            if len(predictions_face) != 0:\n",
    "                prediction_face = predictions_face[idx]\n",
    "                for pred in prediction_face:\n",
    "                    xmin = int(max(pred[0] / width_ratio, 0))\n",
    "                    ymin = int(max(pred[1] / height_ratio, 0))\n",
    "                    xmax = int(min((pred[2]) / width_ratio, width))\n",
    "                    ymax = int(min((pred[3]) / height_ratio, height))\n",
    "                    color = colors[FaceCLS.index(pred[5])]\n",
    "                    \n",
    "                    cv2.rectangle(output_image, (xmin, ymin),\n",
    "                                  (xmax, ymax), color, 2)\n",
    "                    text_size = cv2.getTextSize(\n",
    "                        pred[5] + ' : %.2f' % pred[4],\n",
    "                        cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n",
    "                    cv2.rectangle(\n",
    "                        output_image,\n",
    "                        (xmin, ymin),\n",
    "                        (xmin + text_size[0] + 100,\n",
    "                         ymin + text_size[1] + 20), color, -1)\n",
    "                    cv2.putText(\n",
    "                        output_image, pred[5] + ' : %.2f' % pred[4],\n",
    "                        (xmin, ymin + text_size[1] + 4),\n",
    "                        cv2.FONT_HERSHEY_PLAIN, 1,\n",
    "                        (255, 255, 255), 1)\n",
    "                    \n",
    "                    # save detection results\n",
    "                    pred_cls = pred[5]\n",
    "                    cat_pred = '%s %s %s %s %s %s\\n' % (\n",
    "                        pred_cls,\n",
    "                        str(pred[4]),\n",
    "                        str(xmin), str(ymin), str(xmax), str(ymax))\n",
    "                    \n",
    "                    print(\"face_pred:{}\".format(cat_pred))\n",
    "                    print(\"detected {}\".format(\n",
    "                        save_dir + \"{}\".format(f_file)))\n",
    "            else:\n",
    "                print(\"non-detected {}\".format(\n",
    "                        save_dir + \"{}\".format(f_file)))\n",
    "            # save output image  \n",
    "            cv2.imwrite(save_dir + \"{}\".format(f_file), output_image)\n",
    "            # save images\n",
    "            plt_output_image = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)\n",
    "            plt.figure(figsize=(8,8))\n",
    "            plt.imshow(plt_output_image.astype('uint8'))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            #*****************************************\n",
    "            frm_name = \"episode_{:02d}_scene_{:03d}_frame_{:04d}\".format(episode, scene, idx)\n",
    "            save_file = save_dir + frm_name\n",
    "            graph(episode, scene, idx, graph_info, save_file)\n",
    "            #*****************************************\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
